{
  "task": "ngrams",
  "vocab_size": 4,
  "seq_length": 8,
  "ngram_length": 2,
  "normalize_rewards": true,
  "temperature": 1.0,
  "sampling_strategy": "categorical",
  "preference_distribution": "dirichlet",
  "dirichlet_alpha": 1.5,
  "num_preferences_per_batch": 16,
  "max_iterations": 8000,
  "batch_size": 128,
  "optimizer": "adam",
  "learning_rate": 0.001,
  "gradient_clip": 10.0,
  "eval_every": 500,
  "eval_samples": 1000,
  "final_eval_samples": 10000,
  "num_seeds": 5,
  "base_seed": 42,
  "condition_name": "medium_subtb",
  "hidden_dim": 128,
  "num_layers": 4,
  "conditioning": "concat",
  "activation": "relu",
  "num_parameters": "~70K",
  "factor_capacity": "medium",
  "capacity_level": "medium",
  "loss_function": "subtrajectory_balance",
  "loss_params": {
    "lambda_": 0.9,
    "log_reward_clip": 10.0
  },
  "regularization": "none",
  "regularization_params": {},
  "modifications": "none",
  "modifications_params": {},
  "factor_loss": "subtb",
  "loss_level": "subtb",
  "seed": 42,
  "exp_name": "medium_subtb_seed42"
}