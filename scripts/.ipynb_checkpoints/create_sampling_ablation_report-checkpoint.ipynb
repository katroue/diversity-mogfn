{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Ablation Study Analysis\n",
    "\n",
    "This notebook analyzes the sampling ablation study results to identify the best configurations for:\n",
    "1. Exploration Temperature\n",
    "2. Sampling Strategies\n",
    "3. On-Policy vs Off-Policy\n",
    "4. Preference Diversity\n",
    "5. Batch Size Effects\n",
    "\n",
    "**Inputs:**\n",
    "- `results/ablations/sampling/all_results.csv`\n",
    "- `configs/ablations/sampling_ablation.yaml`\n",
    "\n",
    "**Outputs:**\n",
    "- Summary reports for each experiment type\n",
    "- Comparative visualizations\n",
    "- Best configuration recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_path = 'results/ablations/sampling/all_results.csv'\n",
    "config_path = 'configs/ablations/sampling_ablation.yaml'\n",
    "\n",
    "print(f\"Loading results from: {results_path}\")\n",
    "df = pd.read_csv(results_path)\n",
    "print(f\"‚úì Loaded {len(df)} experiments\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Load configuration\n",
    "print(f\"\\nLoading config from: {config_path}\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('results/ablations/sampling/report')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {output_dir}\")\n",
    "\n",
    "# Define key metrics to analyze\n",
    "key_metrics = [\n",
    "    'hypervolume', 'tds', 'mpd', 'mce', 'pmd', \n",
    "    'pas', 'fci', 'qds', 'der', 'rbd'\n",
    "]\n",
    "\n",
    "# Filter to available metrics\n",
    "available_metrics = [m for m in key_metrics if m in df.columns]\n",
    "print(f\"\\nAvailable metrics for analysis: {available_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Experiment Types\n",
    "\n",
    "Categorize experiments based on their configuration to identify which experiments test which hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify experiment types based on varying parameters\n",
    "def categorize_experiments(df):\n",
    "    \"\"\"\n",
    "    Categorize experiments into different ablation types based on \n",
    "    what parameter is being varied.\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    \n",
    "    # Check which columns exist and vary\n",
    "    param_columns = ['temperature', 'sampling_strategy', 'on_policy', \n",
    "                     'preference_sampling', 'alpha', 'batch_size']\n",
    "    \n",
    "    available_params = [col for col in param_columns if col in df.columns]\n",
    "    \n",
    "    print(\"Available parameter columns:\")\n",
    "    for param in available_params:\n",
    "        unique_vals = df[param].unique()\n",
    "        print(f\"  - {param}: {unique_vals} ({len(unique_vals)} values)\")\n",
    "    \n",
    "    # Categorize based on exp_name patterns if parameters not directly available\n",
    "    if 'exp_name' in df.columns:\n",
    "        df['experiment_type'] = df['exp_name'].apply(lambda x: x.split('_')[0] if isinstance(x, str) else 'unknown')\n",
    "        \n",
    "        # More specific categorization\n",
    "        def get_category(name):\n",
    "            if pd.isna(name):\n",
    "                return 'unknown'\n",
    "            name_lower = str(name).lower()\n",
    "            \n",
    "            if 'temp' in name_lower or 'temperature' in name_lower:\n",
    "                return 'temperature'\n",
    "            elif 'strategy' in name_lower or 'greedy' in name_lower or 'stochastic' in name_lower:\n",
    "                return 'sampling_strategy'\n",
    "            elif 'policy' in name_lower or 'onpolicy' in name_lower or 'offpolicy' in name_lower:\n",
    "                return 'policy_type'\n",
    "            elif 'pref' in name_lower or 'dirichlet' in name_lower or 'uniform' in name_lower:\n",
    "                return 'preference_diversity'\n",
    "            elif 'batch' in name_lower:\n",
    "                return 'batch_size'\n",
    "            else:\n",
    "                return 'other'\n",
    "        \n",
    "        df['experiment_category'] = df['exp_name'].apply(get_category)\n",
    "        \n",
    "        print(\"\\nExperiment categories identified:\")\n",
    "        print(df['experiment_category'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = categorize_experiments(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Exploration Temperature\n",
    "\n",
    "Analyze how temperature affects exploration-exploitation trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter temperature experiments\n",
    "temp_df = df[df['experiment_category'] == 'temperature'].copy()\n",
    "\n",
    "if len(temp_df) > 0:\n",
    "    print(f\"Temperature experiments: {len(temp_df)}\")\n",
    "    print(f\"Unique experiment names: {temp_df['exp_name'].unique()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    temp_summary = temp_df.groupby('exp_name')[available_metrics].agg(['mean', 'std', 'count'])\n",
    "    temp_summary.to_csv(output_dir / 'temperature_summary.csv')\n",
    "    print(f\"\\n‚úì Saved: {output_dir / 'temperature_summary.csv'}\")\n",
    "    \n",
    "    # Display summary\n",
    "    display(temp_summary.round(4))\n",
    "    \n",
    "    # Find best configuration for each metric\n",
    "    print(\"\\nüìä Best Temperature Configuration per Metric:\")\n",
    "    for metric in available_metrics[:5]:  # Top 5 metrics\n",
    "        best_config = temp_summary[(metric, 'mean')].idxmax()\n",
    "        best_value = temp_summary.loc[best_config, (metric, 'mean')]\n",
    "        print(f\"  {metric.upper()}: {best_config} ({best_value:.4f})\")\nelse:\n    print(\"‚ö† No temperature experiments found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects\n",
    "if len(temp_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Temperature Effects on Diversity Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Box plot\n",
    "        temp_df_plot = temp_df[['exp_name', metric]].dropna()\n",
    "        if len(temp_df_plot) > 0:\n",
    "            sns.boxplot(data=temp_df_plot, x='exp_name', y=metric, ax=ax)\n",
    "            ax.set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            ax.set_xlabel('Configuration')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'temperature_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_dir / 'temperature_comparison.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Sampling Strategies\n",
    "\n",
    "Compare different sampling strategies (greedy, stochastic, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter sampling strategy experiments\n",
    "strategy_df = df[df['experiment_category'] == 'sampling_strategy'].copy()\n",
    "\n",
    "if len(strategy_df) > 0:\n",
    "    print(f\"Sampling strategy experiments: {len(strategy_df)}\")\n",
    "    print(f\"Unique experiment names: {strategy_df['exp_name'].unique()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    strategy_summary = strategy_df.groupby('exp_name')[available_metrics].agg(['mean', 'std', 'count'])\n",
    "    strategy_summary.to_csv(output_dir / 'sampling_strategy_summary.csv')\n",
    "    print(f\"\\n‚úì Saved: {output_dir / 'sampling_strategy_summary.csv'}\")\n",
    "    \n",
    "    display(strategy_summary.round(4))\n",
    "    \n",
    "    # Best configuration\n",
    "    print(\"\\nüìä Best Sampling Strategy per Metric:\")\n",
    "    for metric in available_metrics[:5]:\n",
    "        best_config = strategy_summary[(metric, 'mean')].idxmax()\n",
    "        best_value = strategy_summary.loc[best_config, (metric, 'mean')]\n",
    "        print(f\"  {metric.upper()}: {best_config} ({best_value:.4f})\")\nelse:\n    print(\"‚ö† No sampling strategy experiments found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sampling strategy effects\n",
    "if len(strategy_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Sampling Strategy Effects on Diversity Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        strategy_df_plot = strategy_df[['exp_name', metric]].dropna()\n",
    "        if len(strategy_df_plot) > 0:\n",
    "            sns.violinplot(data=strategy_df_plot, x='exp_name', y=metric, ax=ax)\n",
    "            ax.set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            ax.set_xlabel('Strategy')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'sampling_strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_dir / 'sampling_strategy_comparison.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 3: On-Policy vs Off-Policy\n",
    "\n",
    "Compare on-policy and off-policy learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter policy type experiments\n",
    "policy_df = df[df['experiment_category'] == 'policy_type'].copy()\n",
    "\n",
    "if len(policy_df) > 0:\n",
    "    print(f\"Policy type experiments: {len(policy_df)}\")\n",
    "    print(f\"Unique experiment names: {policy_df['exp_name'].unique()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    policy_summary = policy_df.groupby('exp_name')[available_metrics].agg(['mean', 'std', 'count'])\n",
    "    policy_summary.to_csv(output_dir / 'policy_type_summary.csv')\n",
    "    print(f\"\\n‚úì Saved: {output_dir / 'policy_type_summary.csv'}\")\n",
    "    \n",
    "    display(policy_summary.round(4))\n",
    "    \n",
    "    # Best configuration\n",
    "    print(\"\\nüìä Best Policy Type per Metric:\")\n",
    "    for metric in available_metrics[:5]:\n",
    "        best_config = policy_summary[(metric, 'mean')].idxmax()\n",
    "        best_value = policy_summary.loc[best_config, (metric, 'mean')]\n",
    "        print(f\"  {metric.upper()}: {best_config} ({best_value:.4f})\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\nüìà Performance Comparison (On-Policy vs Off-Policy):\")\n",
    "    on_policy = policy_df[policy_df['exp_name'].str.contains('on', case=False, na=False)]\n",
    "    off_policy = policy_df[policy_df['exp_name'].str.contains('off', case=False, na=False)]\n",
    "    \n",
    "    if len(on_policy) > 0 and len(off_policy) > 0:\n",
    "        for metric in available_metrics[:5]:\n",
    "            on_mean = on_policy[metric].mean()\n",
    "            off_mean = off_policy[metric].mean()\n",
    "            diff = ((on_mean - off_mean) / off_mean * 100) if off_mean != 0 else 0\n",
    "            winner = \"On-Policy\" if on_mean > off_mean else \"Off-Policy\"\n",
    "            print(f\"  {metric.upper()}: {winner} wins by {abs(diff):.2f}%\")\nelse:\n    print(\"‚ö† No policy type experiments found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy type effects\n",
    "if len(policy_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('On-Policy vs Off-Policy Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        policy_df_plot = policy_df[['exp_name', metric]].dropna()\n",
    "        if len(policy_df_plot) > 0:\n",
    "            sns.barplot(data=policy_df_plot, x='exp_name', y=metric, ax=ax, ci='sd')\n",
    "            ax.set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            ax.set_xlabel('Policy Type')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'policy_type_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_dir / 'policy_type_comparison.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 4: Preference Diversity\n",
    "\n",
    "Analyze the impact of different preference sampling distributions (Dirichlet vs Uniform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter preference diversity experiments\n",
    "pref_df = df[df['experiment_category'] == 'preference_diversity'].copy()\n",
    "\n",
    "if len(pref_df) > 0:\n",
    "    print(f\"Preference diversity experiments: {len(pref_df)}\")\n",
    "    print(f\"Unique experiment names: {pref_df['exp_name'].unique()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    pref_summary = pref_df.groupby('exp_name')[available_metrics].agg(['mean', 'std', 'count'])\n",
    "    pref_summary.to_csv(output_dir / 'preference_diversity_summary.csv')\n",
    "    print(f\"\\n‚úì Saved: {output_dir / 'preference_diversity_summary.csv'}\")\n",
    "    \n",
    "    display(pref_summary.round(4))\n",
    "    \n",
    "    # Best configuration\n",
    "    print(\"\\nüìä Best Preference Sampling per Metric:\")\n",
    "    for metric in available_metrics[:5]:\n",
    "        best_config = pref_summary[(metric, 'mean')].idxmax()\n",
    "        best_value = pref_summary.loc[best_config, (metric, 'mean')]\n",
    "        print(f\"  {metric.upper()}: {best_config} ({best_value:.4f})\")\n",
    "    \n",
    "    # Analyze alpha parameter effect (if available)\n",
    "    if 'alpha' in pref_df.columns:\n",
    "        print(\"\\nüìà Alpha Parameter Analysis:\")\n",
    "        for alpha_val in sorted(pref_df['alpha'].unique()):\n",
    "            alpha_data = pref_df[pref_df['alpha'] == alpha_val]\n",
    "            print(f\"\\n  Alpha = {alpha_val}:\")\n",
    "            for metric in available_metrics[:3]:\n",
    "                mean_val = alpha_data[metric].mean()\n",
    "                print(f\"    {metric.upper()}: {mean_val:.4f}\")\nelse:\n    print(\"‚ö† No preference diversity experiments found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preference diversity effects\n",
    "if len(pref_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Preference Diversity Effects on Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        pref_df_plot = pref_df[['exp_name', metric]].dropna()\n",
    "        if len(pref_df_plot) > 0:\n",
    "            sns.boxplot(data=pref_df_plot, x='exp_name', y=metric, ax=ax)\n",
    "            ax.set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            ax.set_xlabel('Preference Sampling')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'preference_diversity_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_dir / 'preference_diversity_comparison.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment 5: Batch Size Effects\n",
    "\n",
    "Analyze how batch size affects training dynamics and diversity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter batch size experiments\n",
    "batch_df = df[df['experiment_category'] == 'batch_size'].copy()\n",
    "\n",
    "if len(batch_df) > 0:\n",
    "    print(f\"Batch size experiments: {len(batch_df)}\")\n",
    "    print(f\"Unique experiment names: {batch_df['exp_name'].unique()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    batch_summary = batch_df.groupby('exp_name')[available_metrics].agg(['mean', 'std', 'count'])\n",
    "    batch_summary.to_csv(output_dir / 'batch_size_summary.csv')\n",
    "    print(f\"\\n‚úì Saved: {output_dir / 'batch_size_summary.csv'}\")\n",
    "    \n",
    "    display(batch_summary.round(4))\n",
    "    \n",
    "    # Best configuration\n",
    "    print(\"\\nüìä Best Batch Size per Metric:\")\n",
    "    for metric in available_metrics[:5]:\n",
    "        best_config = batch_summary[(metric, 'mean')].idxmax()\n",
    "        best_value = batch_summary.loc[best_config, (metric, 'mean')]\n",
    "        print(f\"  {metric.upper()}: {best_config} ({best_value:.4f})\")\n",
    "    \n",
    "    # Training efficiency analysis\n",
    "    if 'training_time' in batch_df.columns:\n",
    "        print(\"\\n‚è±Ô∏è Training Efficiency:\")\n",
    "        efficiency_df = batch_df.groupby('exp_name')[['training_time'] + available_metrics[:3]].mean()\n",
    "        display(efficiency_df.round(4))\nelse:\n    print(\"‚ö† No batch size experiments found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch size effects\n",
    "if len(batch_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Batch Size Effects on Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        batch_df_plot = batch_df[['exp_name', metric]].dropna()\n",
    "        if len(batch_df_plot) > 0:\n",
    "            sns.boxplot(data=batch_df_plot, x='exp_name', y=metric, ax=ax)\n",
    "            ax.set_title(f'{metric.upper()}', fontweight='bold')\n",
    "            ax.set_xlabel('Batch Size')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'batch_size_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_dir / 'batch_size_comparison.png'}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overall Best Configuration Analysis\n",
    "\n",
    "Rank all configurations across all experiment types to identify the overall best settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall rankings\n",
    "print(\"üèÜ Overall Best Configurations Across All Experiments\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "overall_summary = df.groupby('exp_name')[available_metrics].mean()\n",
    "\n",
    "# Rank by each metric\n",
    "rankings = {}\n",
    "for metric in available_metrics:\n",
    "    rankings[metric] = overall_summary[metric].sort_values(ascending=False)\n",
    "    print(f\"\\n{metric.upper()} - Top 5 Configurations:\")\n",
    "    for i, (config, value) in enumerate(rankings[metric].head(5).items(), 1):\n",
    "        print(f\"  {i}. {config}: {value:.4f}\")\n",
    "\n",
    "# Save overall rankings\n",
    "overall_summary.to_csv(output_dir / 'overall_configuration_rankings.csv')\n",
    "print(f\"\\n‚úì Saved: {output_dir / 'overall_configuration_rankings.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for top configurations\n",
    "from math import pi\n",
    "\n",
    "# Select top 5 configurations by average rank\n",
    "rank_scores = {}\n",
    "for config in overall_summary.index:\n",
    "    ranks = []\n",
    "    for metric in available_metrics:\n",
    "        rank = rankings[metric].index.get_loc(config) + 1\n",
    "        ranks.append(rank)\n",
    "    rank_scores[config] = np.mean(ranks)\n",
    "\n",
    "top_configs = sorted(rank_scores.items(), key=lambda x: x[1])[:5]\n",
    "print(\"\\nüìä Top 5 Configurations by Average Rank:\")\n",
    "for i, (config, avg_rank) in enumerate(top_configs, 1):\n",
    "    print(f\"  {i}. {config} (Avg Rank: {avg_rank:.2f})\")\n",
    "\n",
    "# Create radar chart\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "# Normalize metrics to 0-1 for comparison\n",
    "normalized_data = {}\n",
    "for metric in available_metrics[:6]:  # Top 6 metrics for clarity\n",
    "    min_val = overall_summary[metric].min()\n",
    "    max_val = overall_summary[metric].max()\n",
    "    normalized_data[metric] = (overall_summary[metric] - min_val) / (max_val - min_val + 1e-10)\n",
    "\n",
    "# Plot top 3 configurations\n",
    "angles = [n / float(len(available_metrics[:6])) * 2 * pi for n in range(len(available_metrics[:6]))]\n",
    "angles += angles[:1]\n",
    "\n",
    "for config, _ in top_configs[:3]:\n",
    "    values = [normalized_data[metric][config] for metric in available_metrics[:6]]\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=config)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([m.upper() for m in available_metrics[:6]])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.set_title('Top 3 Configurations - Normalized Performance', size=14, fontweight='bold', pad=20)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'top_configurations_radar.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úì Saved: {output_dir / 'top_configurations_radar.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Summary Report\n",
    "\n",
    "Generate final recommendations based on all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = []\n",
    "report.append(\"=\"*70)\n",
    "report.append(\"SAMPLING ABLATION STUDY - COMPREHENSIVE REPORT\")\n",
    "report.append(\"=\"*70)\n",
    "report.append(\"\")\n",
    "\n",
    "# Summary by experiment type\n",
    "experiment_types = [\n",
    "    ('temperature', 'Exploration Temperature', temp_df),\n",
    "    ('sampling_strategy', 'Sampling Strategies', strategy_df),\n",
    "    ('policy_type', 'On-Policy vs Off-Policy', policy_df),\n",
    "    ('preference_diversity', 'Preference Diversity', pref_df),\n",
    "    ('batch_size', 'Batch Size Effects', batch_df)\n",
    "]\n",
    "\n",
    "for exp_type, title, exp_df in experiment_types:\n",
    "    if len(exp_df) > 0:\n",
    "        report.append(f\"\\n## {title}\")\n",
    "        report.append(\"-\" * 70)\n",
    "        \n",
    "        # Best configuration for each key metric\n",
    "        summary = exp_df.groupby('exp_name')[available_metrics[:5]].mean()\n",
    "        \n",
    "        for metric in available_metrics[:5]:\n",
    "            best_config = summary[metric].idxmax()\n",
    "            best_value = summary[metric].max()\n",
    "            report.append(f\"  Best {metric.upper()}: {best_config} ({best_value:.4f})\")\n",
    "\n",
    "report.append(\"\\n\")\n",
    "report.append(\"=\"*70)\n",
    "report.append(\"OVERALL RECOMMENDATIONS\")\n",
    "report.append(\"=\"*70)\n",
    "report.append(\"\")\n",
    "report.append(\"Top 5 Configurations by Average Rank:\")\n",
    "for i, (config, avg_rank) in enumerate(top_configs, 1):\n",
    "    report.append(f\"  {i}. {config} (Average Rank: {avg_rank:.2f})\")\n",
    "\n",
    "report.append(\"\\n\")\n",
    "report.append(\"Key Insights:\")\n",
    "report.append(\"  - Review the generated CSVs for detailed statistics\")\n",
    "report.append(\"  - Compare PNG visualizations for metric-specific trends\")\n",
    "report.append(\"  - Consider trade-offs between different metrics based on your goals\")\n",
    "report.append(\"\")\n",
    "report.append(\"=\"*70)\n",
    "\n",
    "# Print report\n",
    "report_text = \"\\n\".join(report)\n",
    "print(report_text)\n",
    "\n",
    "# Save report\n",
    "with open(output_dir / 'comprehensive_report.txt', 'w') as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(f\"\\n‚úì Saved: {output_dir / 'comprehensive_report.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all generated files\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"=\"*70)\n",
    "generated_files = sorted(output_dir.glob('*'))\n",
    "for file in generated_files:\n",
    "    print(f\"  ‚úì {file.name}\")\n",
    "\n",
    "print(f\"\\nTotal: {len(generated_files)} files generated in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "1. **Review the summary CSVs** for detailed statistics on each experiment type\n",
    "2. **Examine the visualizations** to understand metric-specific trends\n",
    "3. **Compare top configurations** using the radar chart\n",
    "4. **Select the best configuration** based on your specific optimization goals\n",
    "5. **Run validation experiments** with the recommended settings\n",
    "\n",
    "**Key Files:**\n",
    "- `temperature_summary.csv` - Temperature ablation results\n",
    "- `sampling_strategy_summary.csv` - Strategy comparison\n",
    "- `policy_type_summary.csv` - On/off-policy comparison\n",
    "- `preference_diversity_summary.csv` - Preference sampling analysis\n",
    "- `batch_size_summary.csv` - Batch size effects\n",
    "- `overall_configuration_rankings.csv` - Complete rankings\n",
    "- `comprehensive_report.txt` - Text summary with recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
