{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Ablation Analysis with Novel Diversity Metrics\n",
    "## Research Framework: Understanding Diversity Mechanisms in Multi-Objective GFlowNets\n",
    "\n",
    "This notebook analyzes **RQ1: What drives diversity in MOGFNs?** - specifically testing **H1a: Hypernetwork capacity is the primary driver**.\n",
    "\n",
    "### Novel Metrics Implemented:\n",
    "- **Spatial Diversity**: MCE (Mode Coverage Entropy), PMD (Pairwise Minimum Distance)\n",
    "- **Objective Space**: PAS (Preference-Aligned Spread), PFS (Pareto Front Smoothness)\n",
    "- **Trajectory**: TDS (Trajectory Diversity Score), MPD (Multi-Path Diversity)\n",
    "- **Flow**: FCI (Flow Concentration Index)\n",
    "- **Dynamics**: RBD (Replay Buffer Diversity)\n",
    "\n",
    "### Analysis Goals:\n",
    "1. Compare capacity configurations using novel diversity metrics\n",
    "2. Identify which metrics capture unique diversity aspects\n",
    "3. Select best capacity for sampling ablation (Week 4)\n",
    "4. Validate that novel metrics provide insights beyond traditional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/user-data/uploads\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mode_coverage_entropy, pairwise_minimum_distance\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobjective\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preference_aligned_spread, pareto_front_smoothness\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrajectory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trajectory_diversity_score, multi_path_diversity\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Install local project package (helps resolve local `src` imports if the project is a package)\n",
    "%pip install -e /Users/katherinedemers/Documents/GitHub/diversity-mogfn\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ensure project root is on sys.path so `src` can be imported\n",
    "import sys\n",
    "project_root = Path('/Users/katherinedemers/Documents/GitHub/diversity-mogfn').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "\tsys.path.insert(0, str(project_root))\n",
    "\n",
    "# Fallback path (keeps previous behavior)\n",
    "if '/mnt/user-data/uploads' not in sys.path:\n",
    "\tsys.path.append('/mnt/user-data/uploads')\n",
    "\n",
    "# Import novel diversity metrics (raise informative error if unavailable)\n",
    "try:\n",
    "\tfrom src.metrics.spatial import mode_coverage_entropy, pairwise_minimum_distance\n",
    "\tfrom src.metrics.objective import preference_aligned_spread, pareto_front_smoothness\n",
    "\tfrom src.metrics.trajectory import trajectory_diversity_score, multi_path_diversity\n",
    "\tfrom src.metrics.flow import flow_concentration_index\n",
    "\tfrom src.metrics.dynamics import replay_buffer_diversity\n",
    "\tprint(\"✓ All imports successful\")\n",
    "\tprint(\"✓ Novel diversity metrics loaded\")\n",
    "except ModuleNotFoundError as e:\n",
    "\traise ModuleNotFoundError(\n",
    "\t\tf\"{e}. Ensure the `src` package exists under {project_root} or /mnt/user-data/uploads, \"\n",
    "\t\t\"or install it as a package (e.g. add a pyproject.toml/setup.py and run `%pip install -e .`).\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Capacity Ablation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results by scanning the capacity folders and reading each `metrics.json`\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Define results root path\n",
    "results_root = Path('/Users/katherinedemers/Documents/GitHub/diversity-mogfn/results/ablations/capacity')\n",
    "\n",
    "print(f'Using results root: {results_root}')\n",
    "\n",
    "# Find all metrics.json files under the results root (recursive)\n",
    "metrics_files = list(results_root.rglob('metrics.json'))\n",
    "print(f'Found {len(metrics_files)} metrics.json files')\n",
    "\n",
    "records = []\n",
    "for mf in metrics_files:\n",
    "    try:\n",
    "        with open(mf, 'r') as fh:\n",
    "            data = json.load(fh)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to load {mf}: {e}')\n",
    "        continue\n",
    "\n",
    "    # Attach provenance\n",
    "    data['_metrics_path'] = str(mf)\n",
    "    data['_result_dir'] = str(mf.parent)\n",
    "    records.append(data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "if len(records) == 0:\n",
    "    print('No metrics.json records found under results root.')\n",
    "    # Create example structure for demonstration\n",
    "    df = pd.DataFrame({\n",
    "        'capacity': ['small', 'medium', 'large', 'xlarge'] * 5,\n",
    "        'conditioning': ['hypernet'] * 20,\n",
    "        'seed': [0, 1, 2, 3, 4] * 4,\n",
    "        'hypervolume': np.random.rand(20) * 0.8 + 0.2,\n",
    "        'avg_pairwise_distance': np.random.rand(20) * 2 + 1,\n",
    "        'training_time': np.random.rand(20) * 1000 + 500,\n",
    "        'num_parameters': [100000, 250000, 500000, 1000000] * 5\n",
    "    })\n",
    "else:\n",
    "    df = pd.json_normalize(records)\n",
    "    # Coerce seed column to integer where possible\n",
    "    if 'seed' in df.columns:\n",
    "        df['seed'] = pd.to_numeric(df['seed'], errors='coerce').astype('Int64')\n",
    "\n",
    "print('\\nLoaded results dataframe with columns:')\n",
    "print(df.columns.tolist())\n",
    "print(f'Number of experiments loaded: {len(df)}')\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nCapacity levels: {df['capacity'].unique()}\")\n",
    "    print(f\"Conditioning types: {df['conditioning'].unique()}\")\n",
    "    print(f\"Seeds: {df['seed'].unique()}\")\n",
    "\n",
    "# Determine available novel metrics (keeps downstream cells compatible)\n",
    "novel_metric_columns = ['mce', 'num_modes', 'pmd', 'pas', 'pfs', 'tds', 'mpd', 'fci', 'rbd']\n",
    "available_novel_metrics = [col for col in novel_metric_columns if col in df.columns]\n",
    "missing_novel_metrics = [col for col in novel_metric_columns if col not in df.columns]\n",
    "\n",
    "print(f'\\nAvailable novel metrics: {available_novel_metrics}')\n",
    "print(f'Missing novel metrics: {missing_novel_metrics}')\n",
    "\n",
    "# Provide a convenience variable for capacity order used in downstream plotting/analysis\n",
    "capacity_order = ['small', 'medium', 'large', 'xlarge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Novel Diversity Metrics\n",
    "\n",
    "**CRITICAL**: Your CSV must contain the raw data needed to compute these metrics:\n",
    "- `trajectories`: List of trajectory objects for each experiment\n",
    "- `objectives`: Objective values array (N, num_objectives)\n",
    "- `state_visits`: State visitation counts (for FCI)\n",
    "- `replay_buffer`: Replay buffer trajectories (for RBD)\n",
    "\n",
    "**If you don't have these yet**, you'll need to:\n",
    "1. Modify your experimental code to save this data\n",
    "2. Or compute metrics during training and save them directly\n",
    "\n",
    "For now, I'll show you how to compute them when data is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_novel_metrics(row):\n",
    "    \"\"\"\n",
    "    Compute all novel diversity metrics for one experimental run.\n",
    "    \n",
    "    This function assumes your CSV has columns with raw data:\n",
    "    - 'objectives': numpy array of shape (N, num_objectives)\n",
    "    - 'trajectories': list of trajectory objects\n",
    "    - 'state_visits': dict or array of state visitation counts\n",
    "    - 'replay_buffer': list of trajectory objects\n",
    "    - 'gflownet': trained GFlowNet model (for PAS)\n",
    "    \n",
    "    Returns dict of computed metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # === SPATIAL DIVERSITY ===\n",
    "    try:\n",
    "        objectives = row['objectives']  # Should be numpy array\n",
    "        mce, num_modes = mode_coverage_entropy(objectives, eps='auto', min_samples=5)\n",
    "        metrics['mce'] = mce\n",
    "        metrics['num_modes'] = num_modes\n",
    "        \n",
    "        pmd = pairwise_minimum_distance(objectives, top_k=100)\n",
    "        metrics['pmd'] = pmd\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute spatial metrics - {e}\")\n",
    "        metrics['mce'] = np.nan\n",
    "        metrics['num_modes'] = np.nan\n",
    "        metrics['pmd'] = np.nan\n",
    "    \n",
    "    # === OBJECTIVE SPACE ===\n",
    "    try:\n",
    "        # PAS requires trained GFlowNet model\n",
    "        if 'gflownet' in row:\n",
    "            pas, spreads = preference_aligned_spread(\n",
    "                row['gflownet'], \n",
    "                num_preferences=20, \n",
    "                samples_per_pref=50\n",
    "            )\n",
    "            metrics['pas'] = pas\n",
    "            metrics['pas_std'] = np.std(spreads)\n",
    "        else:\n",
    "            metrics['pas'] = np.nan\n",
    "            metrics['pas_std'] = np.nan\n",
    "        \n",
    "        # PFS\n",
    "        pfs = pareto_front_smoothness(objectives, method='curve_fitting')\n",
    "        metrics['pfs'] = pfs\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute objective space metrics - {e}\")\n",
    "        metrics['pas'] = np.nan\n",
    "        metrics['pas_std'] = np.nan\n",
    "        metrics['pfs'] = np.nan\n",
    "    \n",
    "    # === TRAJECTORY DIVERSITY ===\n",
    "    try:\n",
    "        trajectories = row['trajectories']  # Should be list of trajectory objects\n",
    "        \n",
    "        tds = trajectory_diversity_score(trajectories)\n",
    "        metrics['tds'] = tds\n",
    "        \n",
    "        mpd = multi_path_diversity(trajectories, method='entropy')\n",
    "        metrics['mpd'] = mpd\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute trajectory metrics - {e}\")\n",
    "        metrics['tds'] = np.nan\n",
    "        metrics['mpd'] = np.nan\n",
    "    \n",
    "    # === FLOW CONCENTRATION ===\n",
    "    try:\n",
    "        state_visits = row['state_visits']  # Dict or array\n",
    "        fci = flow_concentration_index(state_visits, method='gini')\n",
    "        metrics['fci'] = fci\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute flow metrics - {e}\")\n",
    "        metrics['fci'] = np.nan\n",
    "    \n",
    "    # === DYNAMICS ===\n",
    "    try:\n",
    "        replay_buffer = row['replay_buffer']  # List of trajectories\n",
    "        rbd = replay_buffer_diversity(\n",
    "            replay_buffer, \n",
    "            metric='trajectory_distance',\n",
    "            sample_size=500\n",
    "        )\n",
    "        metrics['rbd'] = rbd\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute dynamics metrics - {e}\")\n",
    "        metrics['rbd'] = np.nan\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Apply to all rows (if raw data is available)\n",
    "print(\"Computing novel diversity metrics...\")\n",
    "print(\"⚠️ NOTE: This requires raw data (trajectories, objectives, etc.) in your CSV\")\n",
    "print(\"If you don't have this data yet, skip to the analysis section using traditional metrics\")\n",
    "\n",
    "# Uncomment when you have raw data:\n",
    "# novel_metrics = df.apply(compute_all_novel_metrics, axis=1, result_type='expand')\n",
    "# df = pd.concat([df, novel_metrics], axis=1)\n",
    "# print(\"✓ Novel metrics computed for all experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Load Pre-Computed Novel Metrics\n",
    "\n",
    "If you computed metrics during training and saved them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your CSV already contains novel metrics as columns:\n",
    "novel_metric_columns = ['mce', 'num_modes', 'pmd', 'pas', 'pfs', 'tds', 'mpd', 'fci', 'rbd']\n",
    "\n",
    "# Check which metrics are available\n",
    "available_novel_metrics = [col for col in novel_metric_columns if col in df.columns]\n",
    "missing_novel_metrics = [col for col in novel_metric_columns if col not in df.columns]\n",
    "\n",
    "print(f\"Available novel metrics: {available_novel_metrics}\")\n",
    "print(f\"Missing novel metrics: {missing_novel_metrics}\")\n",
    "\n",
    "if len(available_novel_metrics) == 0:\n",
    "    print(\"\\n⚠️ No novel metrics found in CSV. Using traditional metrics only for now.\")\n",
    "    print(\"You should compute novel metrics during your experiments and save them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics by Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by capacity and compute statistics\n",
    "capacity_stats = df.groupby('capacity').agg({\n",
    "    'hypervolume': ['mean', 'std'],\n",
    "    'avg_pairwise_distance': ['mean', 'std'],\n",
    "    'training_time': ['mean', 'std'],\n",
    "    'num_parameters': 'first',\n",
    "}).round(4)\n",
    "\n",
    "# Add novel metrics if available\n",
    "for metric in available_novel_metrics:\n",
    "    if metric in df.columns:\n",
    "        capacity_stats[(metric, 'mean')] = df.groupby('capacity')[metric].mean()\n",
    "        capacity_stats[(metric, 'std')] = df.groupby('capacity')[metric].std()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CAPACITY ABLATION: DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(capacity_stats)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Novel Metrics vs Capacity\n",
    "\n",
    "### Figure 1: Comprehensive Diversity Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which metrics to plot\n",
    "metrics_to_plot = {\n",
    "    'Traditional': ['hypervolume', 'avg_pairwise_distance', 'spacing', 'spread'],\n",
    "    'Spatial': ['mce', 'pmd'],\n",
    "    'Objective': ['pas', 'pfs'],\n",
    "    'Trajectory': ['tds', 'mpd'],\n",
    "    'Flow/Dynamics': ['fci', 'rbd']\n",
    "}\n",
    "\n",
    "# Filter to only available metrics\n",
    "available_metrics_by_category = {}\n",
    "for category, metrics in metrics_to_plot.items():\n",
    "    available = [m for m in metrics if m in df.columns]\n",
    "    if available:\n",
    "        available_metrics_by_category[category] = available\n",
    "\n",
    "# Create comprehensive figure\n",
    "n_categories = len(available_metrics_by_category)\n",
    "fig, axes = plt.subplots(n_categories, 1, figsize=(12, 5 * n_categories))\n",
    "if n_categories == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "capacity_order = ['small', 'medium', 'large', 'xlarge']\n",
    "\n",
    "for idx, (category, metrics) in enumerate(available_metrics_by_category.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot all metrics in this category\n",
    "    for metric in metrics:\n",
    "        # Compute mean and std for each capacity\n",
    "        means = []\n",
    "        stds = []\n",
    "        for cap in capacity_order:\n",
    "            data = df[df['capacity'] == cap][metric].dropna()\n",
    "            means.append(data.mean())\n",
    "            stds.append(data.std())\n",
    "        \n",
    "        # Plot with error bars\n",
    "        ax.errorbar(capacity_order, means, yerr=stds, \n",
    "                   marker='o', linewidth=2, markersize=8, \n",
    "                   label=metric.upper(), capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Capacity', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Metric Value', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{category} Diversity Metrics vs Capacity', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/capacity_diversity_metrics.pdf', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: capacity_diversity_metrics.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Metric Correlation Heatmap\n",
    "\n",
    "**Research Question**: Do novel metrics capture distinct aspects of diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all diversity metrics (traditional + novel)\n",
    "all_diversity_metrics = ['avg_pairwise_distance', 'spacing', 'spread'] + available_novel_metrics\n",
    "available_for_corr = [m for m in all_diversity_metrics if m in df.columns]\n",
    "\n",
    "if len(available_for_corr) >= 2:\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[available_for_corr].corr()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1, square=True,\n",
    "                cbar_kws={'label': 'Correlation'})\n",
    "    plt.title('Diversity Metrics Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/mnt/user-data/outputs/metric_correlation_heatmap.pdf', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: metric_correlation_heatmap.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Find metrics with low correlation (novel information)\n",
    "    print(\"\\n=== METRIC INDEPENDENCE ANALYSIS ===\")\n",
    "    print(\"Metrics with low correlation (< 0.5) capture distinct diversity aspects:\\n\")\n",
    "    \n",
    "    for i, metric1 in enumerate(available_for_corr):\n",
    "        for metric2 in available_for_corr[i+1:]:\n",
    "            corr = corr_matrix.loc[metric1, metric2]\n",
    "            if abs(corr) < 0.5:\n",
    "                print(f\"  • {metric1.upper()} ↔ {metric2.upper()}: r={corr:.3f}\")\n",
    "else:\n",
    "    print(\"⚠️ Not enough metrics available for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: Capacity vs Parameters (Efficiency Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity-Efficiency Ratio (DER) calculation\n",
    "if 'avg_pairwise_distance' in df.columns:\n",
    "    df['der'] = df['avg_pairwise_distance'] / (df['training_time'] * df['num_parameters'] / 1e9)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # (a) Diversity vs Parameters\n",
    "    for capacity in capacity_order:\n",
    "        subset = df[df['capacity'] == capacity]\n",
    "        axes[0].scatter(subset['num_parameters'], subset['avg_pairwise_distance'],\n",
    "                       label=capacity, s=100, alpha=0.6)\n",
    "    \n",
    "    axes[0].set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Diversity (Avg Pairwise Distance)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('(a) Diversity vs Model Size', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # (b) Diversity-Efficiency Ratio\n",
    "    der_by_capacity = df.groupby('capacity')['der'].agg(['mean', 'std'])\n",
    "    axes[1].bar(capacity_order, \n",
    "               [der_by_capacity.loc[c, 'mean'] if c in der_by_capacity.index else 0 for c in capacity_order],\n",
    "               yerr=[der_by_capacity.loc[c, 'std'] if c in der_by_capacity.index else 0 for c in capacity_order],\n",
    "               capsize=5, alpha=0.7, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    \n",
    "    axes[1].set_xlabel('Capacity', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Diversity-Efficiency Ratio (DER)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('(b) Efficiency Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/mnt/user-data/outputs/capacity_efficiency_analysis.pdf', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: capacity_efficiency_analysis.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "### Hypothesis Testing: Does capacity significantly affect diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTesting H1a: Hypernetwork capacity significantly affects diversity\\n\")\n",
    "\n",
    "# Select primary diversity metric\n",
    "primary_diversity_metric = 'avg_pairwise_distance'\n",
    "if 'mce' in df.columns:\n",
    "    primary_diversity_metric = 'mce'  # Prefer novel metric if available\n",
    "\n",
    "# Prepare data by capacity\n",
    "groups = []\n",
    "for cap in capacity_order:\n",
    "    data = df[df['capacity'] == cap][primary_diversity_metric].dropna()\n",
    "    groups.append(data)\n",
    "\n",
    "# One-way ANOVA\n",
    "if all(len(g) > 0 for g in groups):\n",
    "    F_stat, p_value = stats.f_oneway(*groups)\n",
    "    \n",
    "    print(f\"Metric: {primary_diversity_metric.upper()}\")\n",
    "    print(f\"F-statistic: {F_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4e}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"✓ SIGNIFICANT: Capacity has significant effect on diversity (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"✗ NOT SIGNIFICANT: Capacity effect not detected (p >= 0.05)\")\n",
    "    \n",
    "    # Post-hoc pairwise comparisons (Bonferroni corrected)\n",
    "    print(\"\\n--- Post-hoc Pairwise Comparisons (Bonferroni corrected) ---\")\n",
    "    n_comparisons = 0\n",
    "    for i, cap1 in enumerate(capacity_order):\n",
    "        for cap2 in capacity_order[i+1:]:\n",
    "            n_comparisons += 1\n",
    "    \n",
    "    for i, cap1 in enumerate(capacity_order):\n",
    "        for cap2 in capacity_order[i+1:]:\n",
    "            t_stat, p = stats.ttest_ind(groups[capacity_order.index(cap1)], \n",
    "                                       groups[capacity_order.index(cap2)])\n",
    "            p_corrected = min(p * n_comparisons, 1.0)\n",
    "            sig = \"***\" if p_corrected < 0.001 else \"**\" if p_corrected < 0.01 else \"*\" if p_corrected < 0.05 else \"ns\"\n",
    "            print(f\"  {cap1:8s} vs {cap2:8s}: p={p_corrected:.4f} {sig}\")\n",
    "    \n",
    "    # Effect sizes (Cohen's d)\n",
    "    print(\"\\n--- Effect Sizes (Cohen's d) ---\")\n",
    "    def cohens_d(group1, group2):\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "        pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "        return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "    \n",
    "    for i, cap1 in enumerate(capacity_order):\n",
    "        for cap2 in capacity_order[i+1:]:\n",
    "            d = cohens_d(groups[capacity_order.index(cap1)], \n",
    "                        groups[capacity_order.index(cap2)])\n",
    "            magnitude = \"large\" if abs(d) >= 0.8 else \"medium\" if abs(d) >= 0.5 else \"small\"\n",
    "            print(f\"  {cap1:8s} vs {cap2:8s}: d={d:.3f} ({magnitude})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario-Based Configuration Ranking\n",
    "\n",
    "Following the research framework's recommendation for **multi-scenario analysis**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_metric(series, higher_is_better=True):\n",
    "    \"\"\"Normalize metric to [0, 1] range\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val == min_val:\n",
    "        return pd.Series([0.5] * len(series), index=series.index)\n",
    "    \n",
    "    normalized = (series - min_val) / (max_val - min_val)\n",
    "    if not higher_is_better:\n",
    "        normalized = 1 - normalized\n",
    "    return normalized\n",
    "\n",
    "# Prepare scoring dataframe\n",
    "scoring_df = df.groupby(['capacity', 'conditioning']).agg({\n",
    "    'hypervolume': 'mean',\n",
    "    'avg_pairwise_distance': 'mean',\n",
    "    'training_time': 'mean',\n",
    "    'num_parameters': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add novel metrics if available\n",
    "for metric in available_novel_metrics:\n",
    "    if metric in df.columns:\n",
    "        scoring_df[metric] = df.groupby(['capacity', 'conditioning'])[metric].mean().values\n",
    "\n",
    "# Normalize metrics\n",
    "scoring_df['quality_norm'] = normalize_metric(scoring_df['hypervolume'], True)\n",
    "scoring_df['diversity_norm'] = normalize_metric(scoring_df['avg_pairwise_distance'], True)\n",
    "scoring_df['efficiency_norm'] = normalize_metric(\n",
    "    1 / (scoring_df['training_time'] * scoring_df['num_parameters']), True\n",
    ")\n",
    "\n",
    "# Add normalized novel metrics\n",
    "if 'mce' in scoring_df.columns:\n",
    "    scoring_df['mce_norm'] = normalize_metric(scoring_df['mce'], True)\n",
    "if 'tds' in scoring_df.columns:\n",
    "    scoring_df['tds_norm'] = normalize_metric(scoring_df['tds'], True)\n",
    "if 'fci' in scoring_df.columns:\n",
    "    scoring_df['fci_norm'] = normalize_metric(scoring_df['fci'], False)  # Lower FCI is better\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCENARIO-BASED CONFIGURATION RANKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# === SCENARIO 1: Maximum Diversity (PRIMARY FOR RESEARCH) ===\n",
    "print(\"\\n📊 SCENARIO 1: Maximum Diversity (Research Focus)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Build diversity score using multiple metrics if available\n",
    "diversity_components = {'diversity_norm': 0.30}\n",
    "remaining_weight = 0.50\n",
    "\n",
    "if 'mce_norm' in scoring_df.columns:\n",
    "    diversity_components['mce_norm'] = 0.25\n",
    "    remaining_weight -= 0.25\n",
    "if 'tds_norm' in scoring_df.columns:\n",
    "    diversity_components['tds_norm'] = 0.15\n",
    "    remaining_weight -= 0.15\n",
    "if 'fci_norm' in scoring_df.columns:\n",
    "    diversity_components['fci_norm'] = 0.10\n",
    "    remaining_weight -= 0.10\n",
    "\n",
    "# Distribute remaining weight\n",
    "if remaining_weight > 0:\n",
    "    diversity_components['diversity_norm'] += remaining_weight\n",
    "\n",
    "# Compute diversity-focused score\n",
    "scoring_df['diversity_focused_score'] = sum(\n",
    "    weight * scoring_df[metric] for metric, weight in diversity_components.items()\n",
    ")\n",
    "scoring_df['diversity_focused_score'] += 0.15 * scoring_df['quality_norm']  # Quality threshold\n",
    "scoring_df['diversity_focused_score'] += 0.05 * scoring_df['efficiency_norm']  # Minor efficiency\n",
    "\n",
    "top_3_diversity = scoring_df.nlargest(3, 'diversity_focused_score')\n",
    "print(\"\\nTop 3 Configurations:\")\n",
    "print(top_3_diversity[['capacity', 'conditioning', 'diversity_focused_score', \n",
    "                       'avg_pairwise_distance', 'hypervolume']].to_string(index=False))\n",
    "\n",
    "# === SCENARIO 2: Balanced Quality-Diversity ===\n",
    "print(\"\\n📊 SCENARIO 2: Balanced Quality-Diversity\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "scoring_df['balanced_score'] = (\n",
    "    0.40 * scoring_df['quality_norm'] +\n",
    "    0.40 * scoring_df['diversity_norm'] +\n",
    "    0.20 * scoring_df['efficiency_norm']\n",
    ")\n",
    "\n",
    "top_3_balanced = scoring_df.nlargest(3, 'balanced_score')\n",
    "print(\"\\nTop 3 Configurations:\")\n",
    "print(top_3_balanced[['capacity', 'conditioning', 'balanced_score',\n",
    "                      'avg_pairwise_distance', 'hypervolume']].to_string(index=False))\n",
    "\n",
    "# === SCENARIO 3: Quality Priority (Sanity Check) ===\n",
    "print(\"\\n📊 SCENARIO 3: Quality Priority (Sanity Check)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "scoring_df['quality_priority_score'] = (\n",
    "    0.60 * scoring_df['quality_norm'] +\n",
    "    0.30 * scoring_df['diversity_norm'] +\n",
    "    0.10 * scoring_df['efficiency_norm']\n",
    ")\n",
    "\n",
    "top_3_quality = scoring_df.nlargest(3, 'quality_priority_score')\n",
    "print(\"\\nTop 3 Configurations:\")\n",
    "print(top_3_quality[['capacity', 'conditioning', 'quality_priority_score',\n",
    "                     'hypervolume', 'avg_pairwise_distance']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Recommendation for Sampling Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best configuration for next phase (sampling ablation)\n",
    "best_config = scoring_df.nlargest(1, 'diversity_focused_score').iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 RECOMMENDED CONFIGURATION FOR SAMPLING ABLATION (Week 4)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCapacity: {best_config['capacity'].upper()}\")\n",
    "print(f\"Conditioning: {best_config['conditioning'].upper()}\")\n",
    "print(f\"\\nRationale: Maximizes diversity while maintaining quality\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  • Diversity Score: {best_config['diversity_focused_score']:.4f}\")\n",
    "print(f\"  • Avg Pairwise Distance: {best_config['avg_pairwise_distance']:.4f}\")\n",
    "print(f\"  • Hypervolume: {best_config['hypervolume']:.4f}\")\n",
    "print(f\"  • Parameters: {int(best_config['num_parameters']):,}\")\n",
    "print(f\"  • Training Time: {best_config['training_time']:.2f}s\")\n",
    "\n",
    "if 'mce' in best_config:\n",
    "    print(f\"  • Mode Coverage Entropy (MCE): {best_config['mce']:.4f}\")\n",
    "if 'tds' in best_config:\n",
    "    print(f\"  • Trajectory Diversity Score (TDS): {best_config['tds']:.4f}\")\n",
    "if 'fci' in best_config:\n",
    "    print(f\"  • Flow Concentration Index (FCI): {best_config['fci']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n✅ Phase 1 (Capacity Ablation) Complete\")\n",
    "print(\"➡️  Next: Run sampling ablation experiments with this configuration\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed results\n",
    "scoring_df.to_csv('/mnt/user-data/outputs/capacity_ablation_scores.csv', index=False)\n",
    "print(\"✓ Saved: capacity_ablation_scores.csv\")\n",
    "\n",
    "# Save recommendation\n",
    "with open('/mnt/user-data/outputs/recommended_config.txt', 'w') as f:\n",
    "    f.write(\"RECOMMENDED CONFIGURATION FOR SAMPLING ABLATION\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Capacity: {best_config['capacity']}\\n\")\n",
    "    f.write(f\"Conditioning: {best_config['conditioning']}\\n\")\n",
    "    f.write(f\"\\nDiversity Score: {best_config['diversity_focused_score']:.4f}\\n\")\n",
    "    f.write(f\"Hypervolume: {best_config['hypervolume']:.4f}\\n\")\n",
    "    f.write(f\"Avg Pairwise Distance: {best_config['avg_pairwise_distance']:.4f}\\n\")\n",
    "print(\"✓ Saved: recommended_config.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL RESULTS SAVED TO /mnt/user-data/outputs/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
