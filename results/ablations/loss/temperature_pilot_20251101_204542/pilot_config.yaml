ablation_factors:
  base_loss:
    description: Core GFlowNet training objective
    options:
    - label: TB
      name: trajectory_balance
      params:
        log_reward_clip: 10.0
      type: trajectory_balance
    - label: DB
      name: detailed_balance
      params:
        log_reward_clip: 10.0
      type: detailed_balance
    - label: "SubTB(\u03BB=0.5)"
      name: subtrajectory_balance_05
      params:
        lambda_: 0.5
        log_reward_clip: 10.0
      type: subtrajectory_balance
    - label: "SubTB(\u03BB=0.9)"
      name: subtrajectory_balance_09
      params:
        lambda_: 0.9
        log_reward_clip: 10.0
      type: subtrajectory_balance
    - label: "SubTB(\u03BB=0.95)"
      name: subtrajectory_balance_095
      params:
        lambda_: 0.95
        log_reward_clip: 10.0
      type: subtrajectory_balance
    - label: FM
      name: flow_matching
      params:
        log_reward_clip: 10.0
      type: flow_matching
  modifications:
    description: Modifications to loss computation
    options:
    - label: Standard
      name: standard
      params: {}
      type: none
    - label: "Temp-Scale(\u03C4=2.0)"
      name: temperature_scaled_logits
      params:
        apply_to: logits
        temperature: 2.0
      type: temperature_scaling
    - label: "Reward-Shape(\u03B3=0.1)"
      name: reward_shaping_diversity
      params:
        diversity_metric: novelty
        gamma: 0.1
        shaping_type: diversity_bonus
      type: reward_shaping
  regularization:
    description: Additional terms added to base loss
    options:
    - label: No Reg
      name: none
      params: {}
      type: none
    - label: "Entropy(\u03B2=0.01)"
      name: entropy_001
      params:
        beta: 0.01
        entropy_type: policy
      type: entropy
    - label: "Entropy(\u03B2=0.05)"
      name: entropy_005
      params:
        beta: 0.05
        entropy_type: policy
      type: entropy
    - label: "Entropy(\u03B2=0.1)"
      name: entropy_01
      params:
        beta: 0.1
        entropy_type: policy
      type: entropy
    - label: "Entropy(\u03B2=0.5)"
      name: entropy_05
      params:
        beta: 0.5
        entropy_type: policy
      type: entropy
    - label: "KL-Uniform(\u03B2=0.01)"
      name: kl_uniform_001
      params:
        beta: 0.01
        target: uniform
      type: kl_divergence
    - label: "KL-Uniform(\u03B2=0.1)"
      name: kl_uniform_01
      params:
        beta: 0.1
        target: uniform
      type: kl_divergence
analysis:
  statistical_tests:
  - alpha: 0.05
    groups:
    - base_loss
    metric: mode_coverage_entropy
    name: base_loss_effect
    posthoc: tukey_hsd
    test: one_way_anova
  - comparisons: all_pairs
    metric: mode_coverage_entropy
    name: base_loss_effect_size
    test: cohens_d
  - groups:
    - regularization
    metric: mode_coverage_entropy
    name: regularization_effect
    test: one_way_anova
  - method: pearson
    name: quality_diversity_tradeoff
    test: correlation
    x_metric: hypervolume
    y_metric: mode_coverage_entropy
  visualizations:
  - hue: null
    name: loss_comparison_boxplot
    save: figures/loss_comparison.pdf
    type: boxplot
    x: base_loss
    y: mode_coverage_entropy
  - name: regularization_effect
    save: figures/regularization_effect.pdf
    type: barplot
    x: regularization
    y: mode_coverage_entropy
  - hue: base_loss
    name: quality_diversity_scatter
    save: figures/quality_diversity_tradeoff.pdf
    type: scatter
    x: hypervolume
    y: mode_coverage_entropy
  - hue: base_loss
    name: diversity_evolution
    save: figures/diversity_evolution.pdf
    type: lineplot
    x: iteration
    y: mode_coverage_entropy
  - name: loss_components_heatmap
    pivot:
      columns: regularization
      index: base_loss
      values: mode_coverage_entropy
    save: figures/loss_components_heatmap.pdf
    type: heatmap
deliverables:
  paper_draft:
    figures:
    - 'Figure 5: Quality-diversity tradeoff for different losses'
    key_findings:
    - Best base loss identified
    - Optimal regularization strength
    - Effect sizes quantified
    section: 5.2.3 Loss Function Effects
    tables:
    - 'Table X: Loss function comparison (all metrics)'
  week_5:
  - results/loss_ablation_summary.csv
  - figures/loss_comparison.pdf
  - figures/quality_diversity_tradeoff.pdf
  - figures/regularization_effect.pdf
  - analysis/statistical_tests.txt
  - analysis/best_configuration.yaml
execution:
  checkpointing:
    keep_last_n: 3
    save_every: 1000
  logging:
    local:
      log_level: INFO
      save_plots: true
      save_samples: true
    wandb:
      enabled: true
      entity: your-team
      project: diversity-mogfn
      tags:
      - loss_ablation
      - hypergrid
      - week5
  parallelization:
    max_parallel: 10
    strategy: seed_parallel
  priority:
  - group: base_loss_comparison
  - group: entropy_regularization
  - group: kl_regularization
  - group: subtb_entropy_sweep
  - group: loss_modifications
experiment_name: loss_ablation
experiments:
- base_loss: trajectory_balance
  base_loss_label: TB
  base_loss_params: &id001
    log_reward_clip: 10.0
  base_loss_type: trajectory_balance
  description: TB with temperature=2.0
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: trajectory_balance_t2.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 2.0
- base_loss: trajectory_balance
  base_loss_label: TB
  base_loss_params: *id001
  base_loss_type: trajectory_balance
  description: TB with temperature=5.0
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: trajectory_balance_t5.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 5.0
- base_loss: subtrajectory_balance_09
  base_loss_label: "SubTB(\u03BB=0.9)"
  base_loss_params: &id002
    lambda_: 0.9
    log_reward_clip: 10.0
  base_loss_type: subtrajectory_balance
  description: "SubTB(\u03BB=0.9) with temperature=2.0"
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: subtrajectory_balance_09_t2.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 2.0
- base_loss: subtrajectory_balance_09
  base_loss_label: "SubTB(\u03BB=0.9)"
  base_loss_params: *id002
  base_loss_type: subtrajectory_balance
  description: "SubTB(\u03BB=0.9) with temperature=5.0"
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: subtrajectory_balance_09_t5.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 5.0
- base_loss: detailed_balance
  base_loss_label: DB
  base_loss_params: &id003
    log_reward_clip: 10.0
  base_loss_type: detailed_balance
  description: DB with temperature=2.0
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: detailed_balance_t2.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 2.0
- base_loss: detailed_balance
  base_loss_label: DB
  base_loss_params: *id003
  base_loss_type: detailed_balance
  description: DB with temperature=5.0
  group: temperature_pilot
  modifications: standard
  modifications_params: {}
  modifications_type: none
  name: detailed_balance_t5.0
  regularization: none
  regularization_params: {}
  regularization_type: none
  temperature: 5.0
fixed: &id004
  activation: relu
  arch_type: film
  base_seed: 42
  batch_size: 64
  capacity: medium
  dirichlet_alpha: 1.5
  eval_every: 250
  eval_samples: 500
  final_eval_samples: 2000
  gradient_clip: 10.0
  grid_size:
  - 32
  - 32
  hidden_dim: 128
  learning_rate: 0.01
  lr_schedule: constant
  num_iterations: 1000
  num_layers: 4
  num_preferences_per_batch: 16
  num_seeds: 5
  optimizer: adam
  preference_distribution: dirichlet
  sampling_schedule: uniform
  sampling_strategy: categorical
  seeds:
  - 42
  - 153
  - 264
  - 375
  - 486
  task: hypergrid
  temperature: 2.0
fixed_config: *id004
hypotheses:
  h1:
    prediction: SubTB(0.9) MCE > TB MCE by at least 15%
    rationale: SubTB interpolates between local and global credit assignment
    statement: "SubTB(\u03BB=0.9) produces higher diversity than TB, FM, or DB"
  h2:
    prediction: "entropy(\u03B2=0.05) increases MCE by 20-30%"
    rationale: Encourages policy exploration
    statement: Entropy regularization significantly increases diversity
  h3:
    prediction: "entropy(\u03B2=0.5) reduces hypervolume by >10%"
    rationale: Over-regularization reduces focus on high-reward regions
    statement: Too much regularization hurts quality
  h4:
    prediction: Highest QDS score (balanced quality-diversity)
    rationale: SubTB provides good credit assignment, entropy adds exploration
    statement: "SubTB(0.9) + entropy(\u03B2=0.05) is optimal combination"
  h5:
    prediction: Ranking of losses consistent across different model sizes
    rationale: Loss affects optimization dynamics universally
    statement: Loss function effects are independent of capacity/sampling
integration:
  expected_outcome:
    best_loss: subtrajectory_balance_09
    best_modification: standard
    best_regularization: entropy_005
    combined: "SubTB(0.9) + entropy(\u03B2=0.05)"
  from_previous_ablations:
    capacity_ablation:
      use_in: fixed_config.capacity
      winner: film_medium_64
    sampling_ablation:
      use_in: fixed_config.dirichlet_alpha
      winner: dirichlet_1.5
  to_factorial_experiments:
    export:
    - best_base_loss
    - best_regularization
    - best_modification
    use_in: factorial_experiments/configs/loss_factor.yaml
metrics:
  evaluation:
  - avg_pairwise_distance
  - min_pairwise_distance
  - std_pairwise_distance
  - hypervolume
  - r2_indicator
  - pareto_front_size
  - trajectory_diversity_score
  - mode_coverage_entropy
  - pairwise_minimum_distance
  - preference_aligned_spread
  - pareto_front_smoothness
  - conditional_entropy
  - flow_concentration_index
  - multi_path_diversity
  - diversity_velocity
  - mode_discovery_rate
  - quality_diversity_score
  - diversity_efficiency_ratio
  final:
  - all_evaluation_metrics
  - solution_histogram
  - objective_space_plot
  - trajectory_length_distribution
  - state_visitation_histogram
  training:
  - loss_total
  - loss_base
  - loss_regularization
  - gradient_norm
  - learning_rate
notes:
  adaptive_strategy:
  - If Group 1 shows TB >> SubTB, focus Groups 2-4 on TB only
  - "If entropy clearly helps, expand Group 2 to test \u03B2 \u2208 [0.001, 1.0]"
  - If no regularization helps, investigate why (mode collapse?)
  important:
  - Run Group 1 (base_loss_comparison) FIRST - results inform later groups
  - Update 'best base loss' in Groups 2-5 based on Group 1 results
  - If TB and SubTB(0.9) are close, run both in later groups
  - Monitor training stability - some loss combinations may diverge
  known_issues:
  - DB can be unstable on hypergrid - increase gradient clipping if needed
  - "Very high entropy regularization (\u03B2 > 0.5) may prevent convergence"
  - Reward shaping may need task-specific tuning
  tips:
  - Use early stopping if loss explodes (gradient_norm > 100)
  - Save samples at iterations [1000, 2000, 3000, 4000] for analysis
  - Generate GIFs showing diversity evolution over training
  - Track temperature of policy (entropy) over training
resources:
  compute:
    gpu_type: nvidia_rtx3090
    max_parallel_jobs: 10
    num_gpus_per_job: 1
    platform: slurm_cluster
  storage:
    base_dir: ./experiments/loss_ablation
    structure:
      checkpoints: checkpoints/
      figures: figures/
      logs: logs/
      results: results/
  time_estimates:
    per_config: 2 hours
    per_run: 24 minutes
    total: 70 hours
    with_parallelization: 7 hours
seeds:
- 42
- 153
study_type: ablation
success_criteria:
  excellent:
  - Novel insights discovered (e.g., unexpected interactions)
  - Best loss configuration improves diversity by >25% over baseline
  - Findings consistent with theoretical predictions
  - Results inform factorial experiments
  good:
  - Clear ranking of loss functions established
  - Optimal regularization strength identified
  - Quality-diversity tradeoff quantified
  - All metrics computed successfully
  minimum:
  - All 35 configurations run successfully
  - At least 4/5 seeds complete per configuration
  - Statistical significance (p < 0.05) detected for at least 2 factors
