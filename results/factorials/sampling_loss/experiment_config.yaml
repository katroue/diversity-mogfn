conditions: &id001
- loss: tb
  name: low_tb
  temperature: low
- loss: subtb
  name: low_subtb
  temperature: low
- loss: subtb_entropy
  name: low_subtb_entropy
  temperature: low
- loss: tb
  name: high_tb
  temperature: high
- loss: subtb
  name: high_subtb
  temperature: high
- loss: subtb_entropy
  name: high_subtb_entropy
  temperature: high
- loss: tb
  name: veryhigh_tb
  temperature: very_high
- loss: subtb
  name: veryhigh_subtb
  temperature: very_high
- loss: subtb_entropy
  name: veryhigh_subtb_entropy
  temperature: very_high
config:
  analysis:
    research_questions:
      rq1:
        factor: temperature
        metric: mode_coverage_entropy
        question: Does temperature have a main effect on diversity?
        test: one_way_anova
      rq2:
        factor: loss
        metric: mode_coverage_entropy
        question: Does loss function have a main effect on diversity?
        test: one_way_anova
      rq3:
        factors:
        - temperature
        - loss
        interaction_term: temperature:loss
        metric: mode_coverage_entropy
        question: Is there an interaction between temperature and loss?
        test: two_way_anova
    statistical_tests:
    - alpha: 0.05
      formula: mce ~ C(temperature) + C(loss) + C(temperature):C(loss)
      name: factorial_anova
      test: two_way_anova
    - factor: temperature
      metric: mce
      name: temperature_posthoc
      test: tukey_hsd
    - factor: loss
      metric: mce
      name: loss_posthoc
      test: tukey_hsd
    visualizations:
    - hue: temperature
      name: interaction_plot
      save: figures/sampling_loss_interaction.pdf
      type: interaction_plot
      x: loss
      y: mce
    - name: heatmap
      pivot:
        columns: loss
        index: temperature
        values: mce
      save: figures/sampling_loss_heatmap.pdf
      type: heatmap
  conditions: *id001
  expected_results:
    best_case:
      condition: veryhigh_subtb_entropy
      expected_mce: '>0.5'
      expected_qds: '>0.65'
    efficiency_finding:
      benefit: More stable training with similar diversity
      hypothesis: SubTB+Entropy at low/medium temp matches TB at high temp
      practical_value: Easier to tune, more robust
    scenario_interaction:
      description: Optimal temperature depends on loss function
      examples:
      - 'SubTB+Entropy: works well even at low temp (exploration from entropy)'
      - 'TB: needs high temp to achieve similar diversity'
      implication: Loss function choice affects optimal sampling strategy
    scenario_no_interaction:
      best_combination: veryhigh_subtb_entropy
      description: Temperature and loss have independent additive effects
      implication: Combine best from each ablation
  experiment_name: sampling_loss_2way
  factors:
    loss:
      description: GFlowNet training objective
      levels:
        subtb:
          label: "SubTB(\u03BB=0.9)"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          regularization: none
          regularization_params: {}
        subtb_entropy:
          label: "SubTB(\u03BB=0.9) + Entropy"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          regularization: entropy
          regularization_params:
            beta: 0.01
        tb:
          label: Trajectory Balance
          loss_function: trajectory_balance
          loss_params:
            log_reward_clip: 10.0
          regularization: none
          regularization_params: {}
    temperature:
      description: Exploration temperature
      levels:
        high:
          label: "High (\u03C4=2.0)"
          temperature: 2.0
        low:
          label: "Low (\u03C4=1.0)"
          temperature: 1.0
        very_high:
          label: "Very High (\u03C4=5.0)"
          temperature: 5.0
  fixed:
    activation: relu
    base_seed: 42
    batch_size: 128
    conditioning: concat
    dirichlet_alpha: 1.5
    eval_every: 500
    eval_samples: 1000
    final_eval_samples: 10000
    gradient_clip: 10.0
    grid_size:
    - 32
    - 32
    hidden_dim: 128
    learning_rate: 0.001
    max_iterations: 4000
    num_layers: 4
    num_preferences_per_batch: 16
    num_seeds: 5
    optimizer: adam
    preference_distribution: dirichlet
    sampling_strategy: categorical
    task: hypergrid
  hypotheses:
    h1_temperature_main:
      prediction: Very High > High > Low for all losses
      statement: Higher temperature increases diversity regardless of loss
    h2_loss_main:
      prediction: SubTB+Entropy > SubTB > TB for all temperatures
      statement: SubTB+Entropy provides best diversity regardless of temperature
    h3_interaction:
      expected_pattern: 'SubTB: low temp sufficient, TB: needs high temp'
      prediction: SubTB needs less exploration (better credit assignment), TB needs
        more
      statement: Optimal temperature depends on loss function
    h4_exploration_compensation:
      prediction: SubTB+Entropy works well even at low temperature
      rationale: Entropy regularization provides exploration at loss level
      statement: Entropy regularization reduces need for high temperature
  metrics:
    primary:
    - mode_coverage_entropy
    - trajectory_diversity_score
    - hypervolume
    - quality_diversity_score
    secondary:
    - preference_aligned_spread
    - flow_concentration_index
    - final_loss
    - training_time
  next_steps:
    if_interaction_found:
    - Identify efficient combinations (e.g., SubTB+Entropy at lower temp)
    - Recommend based on stability vs performance trade-off
    - 'Paper contribution: Loss function affects optimal exploration strategy'
    if_no_interaction:
    - Use highest performing level from each factor independently
    - 'Expected: Very High temperature + SubTB+Entropy'
    paper_contribution:
    - First study of loss-sampling interaction in GFlowNets
    - Show that credit assignment quality affects exploration needs
    - Practical guidelines for choosing loss+sampling combination
  resources:
    estimated_time_per_run: 24 minutes
    total_runs: 45
    total_time_parallel: 1.8 hours
    total_time_sequential: 18 hours
  study_type: factorial
seeds:
- 42
- 153
- 264
- 375
- 486
timestamp: '2025-11-04T07:14:10.661446'
