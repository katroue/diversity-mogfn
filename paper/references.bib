@misc{jain2023multiobjectivegflownets,
      title={Multi-Objective GFlowNets}, 
      author={Moksh Jain and Sharath Chandra Raparthy and Alex Hernandez-Garcia and Jarrid Rector-Brooks and Yoshua Bengio and Santiago Miret and Emmanuel Bengio},
      year={2023},
      eprint={2210.12765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.12765}, 
}

@misc{zhu2023sampleefficientmultiobjectivemolecularoptimization,
      title={Sample-efficient Multi-objective Molecular Optimization with GFlowNets}, 
      author={Yiheng Zhu and Jialu Wu and Chaowen Hu and Jiahuan Yan and Chang-Yu Hsieh and Tingjun Hou and Jian Wu},
      year={2023},
      eprint={2302.04040},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.04040}, 
}

@misc{bengio2023gflownetfoundations,
      title={GFlowNet Foundations}, 
      author={Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
      year={2023},
      eprint={2111.09266},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09266}, 
}

@misc{malkin2023trajectorybalanceimprovedcredit,
      title={Trajectory balance: Improved credit assignment in GFlowNets}, 
      author={Nikolay Malkin and Moksh Jain and Emmanuel Bengio and Chen Sun and Yoshua Bengio},
      year={2023},
      eprint={2201.13259},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.13259}, 
}

@misc{hu2024squarederrorexploringloss,
      title={Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks}, 
      author={Rui Hu and Yifan Zhang and Zhuoran Li and Longbo Huang},
      year={2024},
      eprint={2410.02596},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02596}, 
}

@misc{chen2025gradientbasedmultiobjectivedeeplearning,
      title={Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond}, 
      author={Weiyu Chen and Baijiong Lin and Xiaoyuan Zhang and Xi Lin and Han Zhao and Qingfu Zhang and James T. Kwok},
      year={2025},
      eprint={2501.10945},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.10945}, 
}

@misc{lin2024smoothtchebycheffscalarizationmultiobjective,
      title={Smooth Tchebycheff Scalarization for Multi-Objective Optimization}, 
      author={Xi Lin and Xiaoyuan Zhang and Zhiyuan Yang and Fei Liu and Zhenkun Wang and Qingfu Zhang},
      year={2024},
      eprint={2402.19078},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19078}, 
}

@misc{ye2024datadrivenpreferencesamplingpareto,
      title={Data-Driven Preference Sampling for Pareto Front Learning}, 
      author={Rongguang Ye and Lei Chen and Weiduo Liao and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2404.08397},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.08397}, 
}

@misc{dimitriadis2025paretolowrankadaptersefficient,
      title={Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences}, 
      author={Nikolaos Dimitriadis and Pascal Frossard and Francois Fleuret},
      year={2025},
      eprint={2407.08056},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08056}, 
}

@misc{ye2024paretoshapeagnosticparetoset,
      title={Pareto Front Shape-Agnostic Pareto Set Learning in Multi-Objective Optimization}, 
      author={Rongguang Ye and Longcan Chen and Wei-Bin Kou and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2408.05778},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05778}, 
}

@article{Guerreiro_2021,
   title={The Hypervolume Indicator: Computational Problems and Algorithms},
   volume={54},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3453474},
   DOI={10.1145/3453474},
   number={6},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Guerreiro, Andreia P. and Fonseca, Carlos M. and Paquete, Luís},
   year={2021},
   month=jul, pages={1–42} }

@misc{schäpermeier2025r2v2paretocompliantr2,
      title={R2 v2: The Pareto-compliant R2 Indicator for Better Benchmarking in Bi-objective Optimization}, 
      author={Lennart Schäpermeier and Pascal Kerschke},
      year={2025},
      eprint={2407.01504},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2407.01504}, 
}

@InProceedings{pmlr-v202-madan23a,
  title = 	 {Learning {GF}low{N}ets From Partial Episodes For Improved Convergence And Stability},
  author =       {Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Nica, Andrei Cristian and Bosc, Tom and Bengio, Yoshua and Malkin, Nikolay},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {23467--23483},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/madan23a/madan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/madan23a.html},
  abstract = 	 {Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\lambda$) algorithm in reinforcement learning, we introduce <em>subtrajectory balance</em> or SubTB($\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.}
}


% Key GFlowNet Papers
@inproceedings{bengio2021flow,
  title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author={Bengio, Emmanuel and others},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{jain2023multi,
  title={Multi-Objective {GFlowNets}},
  author={Jain, Moksh and others},
  booktitle={ICML},
  year={2023}
}

% Quality-Diversity
@article{pugh2016quality,
  title={Quality Diversity: A New Frontier for Evolutionary Computation},
  author={Pugh, Justin K and others},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016}
}

@ARTICLE{7959075,
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Quality and Diversity Optimization: A Unifying Modular Framework}, 
  year={2018},
  volume={22},
  number={2},
  pages={245-259},
  keywords={Optimization;Legged locomotion;Sociology;Statistics;Algorithm design and analysis;Evolutionary computation;Behavioral diversity;collection of solutions;novelty search;optimization methods;quality-diversity (QD)},
  doi={10.1109/TEVC.2017.2704781}}


% Information Theory
@book{cover2006elements,
  title={Elements of Information Theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2006},
  publisher={Wiley}
}

% Edit Distance
@article{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  journal={Soviet Physics Doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966}
}

% Gini Coefficient
@article{gini1912variability,
  title={Variability and Mutability},
  author={Gini, Corrado},
  journal={Memorie di metodologica statistica},
  year={1912}
}

% Traditional MOO Metrics
@article{zitzler1999multiobjective,
  title={Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  author={Zitzler, Eckart and Thiele, Lothar},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={3},
  number={4},
  pages={257--271},
  year={1999}
}

@article{deb2002fast,
  title={A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
  author={Deb, Kalyanmoy and others},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002}
}