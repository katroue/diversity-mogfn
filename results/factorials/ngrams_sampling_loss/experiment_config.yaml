conditions: &id001
- loss: tb
  name: low_tb
  temperature: low
- loss: subtb
  name: low_subtb
  temperature: low
- loss: subtb_entropy
  name: low_subtb_entropy
  temperature: low
- loss: tb
  name: high_tb
  temperature: high
- loss: subtb
  name: high_subtb
  temperature: high
- loss: subtb_entropy
  name: high_subtb_entropy
  temperature: high
- loss: tb
  name: veryhigh_tb
  temperature: very_high
- loss: subtb
  name: veryhigh_subtb
  temperature: very_high
- loss: subtb_entropy
  name: veryhigh_subtb_entropy
  temperature: very_high
config:
  analysis:
    research_questions:
      rq1:
        factor: temperature
        metric: mode_coverage_entropy
        question: Does temperature have a main effect on diversity?
        test: one_way_anova
      rq2:
        factor: loss
        metric: mode_coverage_entropy
        question: Does loss function have a main effect on diversity?
        test: one_way_anova
      rq3:
        factors:
        - temperature
        - loss
        interaction_term: temperature:loss
        metric: mode_coverage_entropy
        question: Is there an interaction between temperature and loss?
        test: two_way_anova
    statistical_tests:
    - alpha: 0.05
      formula: mce ~ C(temperature) + C(loss) + C(temperature):C(loss)
      name: factorial_anova
      test: two_way_anova
    - by_factors:
      - temperature
      - loss
      description: Check quality-diversity tradeoff
      name: quality_tradeoff
      test: correlation
      variables:
      - hypervolume
      - mce
  conditions: *id001
  expected_results:
    best_case:
      condition: high_subtb_entropy
      expected_mce: '>0.45'
      expected_qds: '>0.65'
      rationale: Balanced exploration + best loss + intrinsic exploration
    interaction_examples:
      scenario_1:
        description: SubTB+Entropy less sensitive to temperature
        interpretation: Entropy regularization compensates for low temperature
        pattern: flat line across temperatures for SubTB+Entropy, increasing line
          for TB
      scenario_2:
        description: Very high temperature hurts quality for all losses
        interpretation: Too much exploration sacrifices quality
        pattern: hypervolume decreases at very_high temperature
    worst_case:
      condition: low_tb
      expected_mce: <0.2
      expected_qds: <0.4
      rationale: Limited exploration + simple loss
  experiment_name: ngrams_sampling_loss_2way
  factors:
    loss:
      description: GFlowNet training objective
      levels:
        subtb:
          label: "SubTB(\u03BB=0.9)"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: none
          regularization_params: {}
        subtb_entropy:
          label: "SubTB(\u03BB=0.9) + Entropy"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: entropy
          regularization_params:
            beta: 0.01
        tb:
          label: Trajectory Balance
          loss_function: trajectory_balance
          loss_params:
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: none
          regularization_params: {}
    temperature:
      description: Exploration temperature for character selection
      levels:
        high:
          description: Increased exploration
          label: "High (\u03C4=2.0)"
          temperature: 2.0
        low:
          description: Standard softmax sampling
          label: "Low (\u03C4=1.0)"
          temperature: 1.0
        very_high:
          description: Maximum exploration (nearly uniform)
          label: "Very High (\u03C4=5.0)"
          temperature: 5.0
  fixed:
    activation: relu
    base_seed: 42
    batch_size: 128
    conditioning: concat
    dirichlet_alpha: 1.5
    eval_every: 500
    eval_samples: 1000
    final_eval_samples: 10000
    gradient_clip: 10.0
    hidden_dim: 128
    learning_rate: 0.001
    max_iterations: 8000
    ngram_length: 2
    normalize_rewards: true
    num_layers: 4
    num_preferences_per_batch: 16
    num_seeds: 5
    optimizer: adam
    preference_distribution: dirichlet
    sampling_strategy: categorical
    seq_length: 8
    task: ngrams
    vocab_size: 4
  hypotheses:
    h1_main_temperature:
      prediction: "High > Low, VeryHigh \u2248 High (diminishing returns)"
      rationale: Higher temperature increases exploration in discrete action space
      statement: Temperature has a significant main effect on diversity
    h2_main_loss:
      prediction: SubTB+Entropy > SubTB > TB
      rationale: SubTB + entropy provides better credit assignment and exploration
      statement: Loss function has a significant main effect on diversity
    h3_interaction:
      prediction: TB benefits more from high temperature; SubTB+Entropy works well
        even at low temperature
      rationale: Better credit assignment reduces need for exploration
      statement: There is an interaction between temperature and loss
    h4_entropy_temperature:
      prediction: "SubTB+Entropy: Low \u2248 High, TB: High >> Low"
      rationale: Entropy provides intrinsic exploration, reducing dependence on temperature
      statement: Entropy regularization reduces need for high temperature
    h5_discrete_space:
      prediction: Larger differences between temperature levels for N-grams
      rationale: Discrete action space more sensitive to temperature scaling
      statement: N-grams shows stronger temperature effects than HyperGrid
  metrics:
    primary:
    - mode_coverage_entropy
    - preference_aligned_spread
    - trajectory_diversity_score
    - quality_diversity_score
    secondary:
    - hypervolume
    - final_loss
    - flow_concentration_index
  notes:
    interpretation:
    - 'If interaction found: Loss function choice depends on exploration budget'
    - 'If no interaction: Can independently optimize loss and temperature'
    - Entropy regularization may be particularly effective for discrete spaces
    key_comparisons:
    - Compare with HyperGrid sampling_loss to identify environment-specific effects
    - Temperature effect may be stronger for N-grams (discrete actions)
    - Watch for quality-diversity tradeoff at very_high temperature
    practical_implications:
    - "If SubTB+Entropy robust to temperature: Use it with standard \u03C4=1.0"
    - 'If TB requires high temperature: SubTB+Entropy is more practical choice'
    - Very high temperature likely to hurt quality without improving diversity much
  resources:
    estimated_time_per_run: 30 minutes
    recommended_parallelization:
      max_parallel: 10
      strategy: condition_parallel
    storage_per_run: ~30 MB
    total_runs: 45
    total_storage: ~1.4 GB
    total_time_parallel: 2.3 hours
    total_time_sequential: 23 hours
  study_type: factorial
seeds:
- 42
- 153
- 264
- 375
- 486
timestamp: '2025-11-08T23:24:11.243539'
