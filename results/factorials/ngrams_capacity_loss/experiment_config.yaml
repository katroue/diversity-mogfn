conditions: &id001
- capacity: small
  loss: tb
  name: small_tb
- capacity: small
  loss: subtb
  name: small_subtb
- capacity: small
  loss: subtb_entropy
  name: small_subtb_entropy
- capacity: medium
  loss: tb
  name: medium_tb
- capacity: medium
  loss: subtb
  name: medium_subtb
- capacity: medium
  loss: subtb_entropy
  name: medium_subtb_entropy
- capacity: large
  loss: tb
  name: large_tb
- capacity: large
  loss: subtb
  name: large_subtb
- capacity: large
  loss: subtb_entropy
  name: large_subtb_entropy
config:
  analysis:
    research_questions:
      rq1:
        factor: capacity
        metric: mode_coverage_entropy
        question: Does capacity have a main effect on diversity?
        test: one_way_anova
      rq2:
        factor: loss
        metric: mode_coverage_entropy
        question: Does loss function have a main effect on diversity?
        test: one_way_anova
      rq3:
        factors:
        - capacity
        - loss
        interaction_term: capacity:loss
        metric: mode_coverage_entropy
        question: Is there an interaction between capacity and loss?
        test: two_way_anova
    statistical_tests:
    - alpha: 0.05
      formula: mce ~ C(capacity) + C(loss) + C(capacity):C(loss)
      name: factorial_anova
      test: two_way_anova
    - description: Check if final_loss differs by capacity or loss type
      formula: final_loss ~ C(capacity) + C(loss)
      name: convergence_analysis
      test: two_way_anova
  conditions: *id001
  expected_results:
    best_case:
      condition: large_subtb_entropy
      expected_mce: '>0.45'
      expected_qds: '>0.65'
      rationale: Large capacity + best loss + exploration regularization
    interaction_example:
      interpretation: Loss function complexity only helps when capacity is sufficient
      scenario: Small models show little difference between losses, large models show
        large differences
    worst_case:
      condition: small_tb
      expected_mce: <0.15
      expected_qds: <0.35
      rationale: Limited capacity + simple loss = mode collapse risk
  experiment_name: ngrams_capacity_loss_2way
  factors:
    capacity:
      description: "Model size (hidden dim \xD7 num layers)"
      levels:
        large:
          activation: relu
          conditioning: concat
          hidden_dim: 256
          label: "Large (256\xD76)"
          num_layers: 6
          num_parameters: ~530K
        medium:
          activation: relu
          conditioning: concat
          hidden_dim: 128
          label: "Medium (128\xD74)"
          num_layers: 4
          num_parameters: ~70K
        small:
          activation: relu
          conditioning: concat
          hidden_dim: 64
          label: "Small (64\xD72)"
          num_layers: 2
          num_parameters: ~1.3K
    loss:
      description: GFlowNet training objective
      levels:
        subtb:
          label: "SubTB(\u03BB=0.9)"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: none
          regularization_params: {}
        subtb_entropy:
          label: "SubTB(\u03BB=0.9) + Entropy"
          loss_function: subtrajectory_balance
          loss_params:
            lambda_: 0.9
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: entropy
          regularization_params:
            beta: 0.01
        tb:
          label: Trajectory Balance
          loss_function: trajectory_balance
          loss_params:
            log_reward_clip: 10.0
          modifications: none
          modifications_params: {}
          regularization: none
          regularization_params: {}
  fixed:
    base_seed: 42
    batch_size: 128
    dirichlet_alpha: 1.5
    eval_every: 500
    eval_samples: 1000
    final_eval_samples: 10000
    gradient_clip: 10.0
    learning_rate: 0.001
    max_iterations: 8000
    ngram_length: 2
    normalize_rewards: true
    num_preferences_per_batch: 16
    num_seeds: 5
    optimizer: adam
    preference_distribution: dirichlet
    sampling_strategy: categorical
    seq_length: 8
    task: ngrams
    temperature: 2.0
    vocab_size: 4
  hypotheses:
    h1_main_capacity:
      prediction: Large > Medium > Small
      rationale: More capacity enables learning diverse policies
      statement: Model capacity has a significant main effect on diversity
    h2_main_loss:
      prediction: SubTB+Entropy > SubTB > TB
      rationale: SubTB provides better credit assignment, entropy promotes exploration
      statement: Loss function has a significant main effect on diversity
    h3_interaction:
      prediction: "Small models: TB \u2248 SubTB (can't leverage complexity), Large\
        \ models: SubTB+Entropy >> TB"
      rationale: Complex losses require capacity to be effective
      statement: There is an interaction between capacity and loss
    h4_discrete_credit:
      prediction: Stronger SubTB effect due to longer trajectories (seq_length steps)
      rationale: Better credit assignment more critical for longer discrete sequences
      statement: SubTB advantage is larger for N-grams than HyperGrid
  metrics:
    primary:
    - mode_coverage_entropy
    - preference_aligned_spread
    - trajectory_diversity_score
    - quality_diversity_score
    secondary:
    - hypervolume
    - final_loss
    - training_time
  notes:
    interpretation:
    - Watch for mode collapse in small_tb condition
    - SubTB should reduce variance in training (check final_loss std)
    - Entropy effect may be stronger for N-grams (discrete actions)
    key_comparisons:
    - Compare with HyperGrid capacity_loss to identify environment-specific effects
    - SubTB may show larger advantage due to longer trajectories (seq_length vs ~10
      steps)
    - Entropy regularization critical for discrete action space exploration
  resources:
    estimated_time_per_run: 30 minutes
    recommended_parallelization:
      max_parallel: 10
      strategy: condition_parallel
    storage_per_run: ~30 MB
    total_runs: 45
    total_storage: ~1.4 GB
    total_time_parallel: 2.3 hours
    total_time_sequential: 23 hours
  study_type: factorial
seeds:
- 42
- 153
- 264
- 375
- 486
timestamp: '2025-11-09T12:44:57.802951'
