# ============================================================================
# LOSS ABLATION STUDY CONFIGURATION
# ============================================================================
# Purpose: Systematically test how different loss functions and regularization
#          affect diversity in Multi-Objective GFlowNets
#
# Week: 5 (after capacity and sampling ablations)
# Duration: ~7 days
# Total Experiments: ~35 configurations × 5 seeds = 175 runs
# ============================================================================

experiment_name: "loss_ablation"
study_type: "ablation"  # One factor at a time

# ============================================================================
# CONTROLLED VARIABLES (FIXED ACROSS ALL EXPERIMENTS)
# ============================================================================
# Based on best/baseline from previous ablations

fixed:
  # Task
  task: "hypergrid"
  grid_size: [32, 32]
  
  # Model Architecture (FIXED from capacity ablation)
  capacity: "medium"  # 3-layer, 64 hidden units
  arch_type: "concat"   # FiLM conditioning (baseline)
  hidden_dim: 128
  num_layers: 4
  activation: "relu"
  
  # Preference Sampling (FIXED from sampling ablation)
  preference_distribution: "dirichlet"
  temperature: 2.0  # Best from sampling ablation
  sampling_strategy: "categorical"
  dirichlet_alpha: 1.5  # Baseline value
  num_preferences_per_batch: 16
  sampling_schedule: "uniform"  # No curriculum
  
  # Training
  num_iterations: 4000
  batch_size: 128
  optimizer: "adam"
  learning_rate: 0.001
  lr_schedule: "constant"
  gradient_clip: 10.0
  
  # Evaluation
  eval_every: 500
  eval_samples: 1000
  final_eval_samples: 10000
  
  # Seeds
  num_seeds: 5
  base_seed: 42

# ============================================================================
# EXPERIMENTAL VARIABLES (LOSS FUNCTIONS - WHAT WE'RE TESTING)
# ============================================================================

ablation_factors:
  # -------------------------------------------------------------------------
  # Factor 1: Base GFlowNet Loss
  # -------------------------------------------------------------------------
  base_loss:
    description: "Core GFlowNet training objective"
    options:
      
      - name: "trajectory_balance"
        label: "TB"
        type: "trajectory_balance"
        params:
          log_reward_clip: 10.0
        
      - name: "detailed_balance"
        label: "DB"
        type: "detailed_balance"
        params:
          log_reward_clip: 10.0
          
      - name: "subtrajectory_balance_05"
        label: "SubTB(λ=0.5)"
        type: "subtrajectory_balance"
        params:
          lambda_: 0.5
          log_reward_clip: 10.0
          
      - name: "subtrajectory_balance_09"
        label: "SubTB(λ=0.9)"
        type: "subtrajectory_balance"
        params:
          lambda_: 0.9
          log_reward_clip: 10.0
          
      - name: "subtrajectory_balance_095"
        label: "SubTB(λ=0.95)"
        type: "subtrajectory_balance"
        params:
          lambda_: 0.95
          log_reward_clip: 10.0
          
      - name: "flow_matching"
        label: "FM"
        type: "flow_matching"
        params:
          log_reward_clip: 10.0

  # -------------------------------------------------------------------------
  # Factor 2: Regularization Terms
  # -------------------------------------------------------------------------
  regularization:
    description: "Additional terms added to base loss"
    options:
      
      - name: "none"
        label: "No Reg"
        type: "none"
        params: {}
        
      - name: "entropy_001"
        label: "Entropy(β=0.01)"
        type: "entropy"
        params:
          beta: 0.01
          entropy_type: "policy"  # H(π(a|s))
          
      - name: "entropy_005"
        label: "Entropy(β=0.05)"
        type: "entropy"
        params:
          beta: 0.05
          entropy_type: "policy"
          
      - name: "entropy_01"
        label: "Entropy(β=0.1)"
        type: "entropy"
        params:
          beta: 0.1
          entropy_type: "policy"
          
      - name: "entropy_05"
        label: "Entropy(β=0.5)"
        type: "entropy"
        params:
          beta: 0.5
          entropy_type: "policy"
          
      - name: "kl_uniform_001"
        label: "KL-Uniform(β=0.01)"
        type: "kl_divergence"
        params:
          beta: 0.01
          target: "uniform"  # KL(π || uniform)
          
      - name: "kl_uniform_01"
        label: "KL-Uniform(β=0.1)"
        type: "kl_divergence"
        params:
          beta: 0.1
          target: "uniform"

  # -------------------------------------------------------------------------
  # Factor 3: Loss Modifications
  # -------------------------------------------------------------------------
  modifications:
    description: "Modifications to loss computation"
    options:
      
      - name: "standard"
        label: "Standard"
        type: "none"
        params: {}
        
      - name: "temperature_scaled_logits"
        label: "Temp-Scale(τ=2.0)"
        type: "temperature_scaling"
        params:
          temperature: 2.0
          apply_to: "logits"  # Scale logits before softmax
          
      - name: "reward_shaping_diversity"
        label: "Reward-Shape(γ=0.1)"
        type: "reward_shaping"
        params:
          gamma: 0.1
          shaping_type: "diversity_bonus"
          diversity_metric: "novelty"  # Bonus for novel solutions

# ============================================================================
# EXPERIMENT CONFIGURATIONS
# ============================================================================
# Each configuration tests ONE aspect while keeping others at baseline

experiments:
  
  # ---------------------------------------------------------------------------
  # Experiment Group 1: Base Loss Functions (NO regularization)
  # ---------------------------------------------------------------------------
  - group: "base_loss_comparison"
    description: "Compare base losses without any regularization"
    fixed:
      regularization: "none"
      modifications: "standard"
    vary:
      base_loss: 
        - "trajectory_balance" 
        - "detailed_balance"
        - "subtrajectory_balance_05"
        - "subtrajectory_balance_09" # best base loss found
        - "subtrajectory_balance_095"
        - "flow_matching"
    # 6 experiments × 5 seeds = 30 runs
    
  # ---------------------------------------------------------------------------
  # Experiment Group 2: Entropy Regularization (on best base loss)
  # ---------------------------------------------------------------------------
  - group: "entropy_regularization"
    description: "Test entropy regularization with different β values"
    fixed:
      base_loss: "subtrajectory_balance_09" # best from group 1
      modifications: "standard"
    vary:
      regularization:
        - "none"
        - "entropy_001" # Best found from summary results
        - "entropy_005"
        - "entropy_01"
        - "entropy_05"
    # 5 experiments × 5 seeds = 25 runs
    
  # ---------------------------------------------------------------------------
  # Experiment Group 3: KL Regularization (on best base loss)
  # ---------------------------------------------------------------------------
  - group: "kl_regularization"
    description: "Test KL divergence to uniform policy"
    fixed:
      base_loss: "subtrajectory_balance_09" # Best from group 1
      modifications: "standard"
    vary:
      regularization:
        - "none"
        - "kl_uniform_001" # found twice as more modes that 01
        - "kl_uniform_01" # Best found but has minimal impact on diversity according to results
    # 3 experiments × 5 seeds = 15 runs
    
  # ---------------------------------------------------------------------------
  # Experiment Group 4: Loss Modifications
  # ---------------------------------------------------------------------------
  - group: "loss_modifications"
    description: "Test modifications on best base+reg combination"
    fixed:
      base_loss: "subtrajectory_balance_09" # Best from Group 1
      regularization: "entropy_001"  # Updated after Group 2 results
    vary:
      modifications:
        - "standard"
        - "temperature_scaled_logits"
        - "reward_shaping_diversity"
    # 3 experiments × 5 seeds = 15 runs

# ============================================================================
# METRICS TO TRACK
# ============================================================================

metrics:
  
  # Training metrics (logged every 100 iterations)
  training:
    - loss_total
    - loss_base
    - loss_regularization
    - gradient_norm
    - learning_rate
    
  # Evaluation metrics (logged every eval_every iterations)
  evaluation:
    
    # Traditional diversity
    - avg_pairwise_distance
    - min_pairwise_distance
    - std_pairwise_distance
    
    # Quality
    - hypervolume
    - r2_indicator
    - pareto_front_size
    
    # Proposed diversity metrics
    - trajectory_diversity_score  # TDS
    - mode_coverage_entropy       # MCE
    - pairwise_minimum_distance   # PMD
    - preference_aligned_spread   # PAS
    - pareto_front_smoothness     # PFS
    - conditional_entropy         # CE
    
    # GFlowNet-specific
    - flow_concentration_index    # FCI
    - multi_path_diversity        # MPD
    
    # Training dynamics
    - diversity_velocity          # DV
    - mode_discovery_rate         # MDR
    
    # Composite
    - quality_diversity_score     # QDS (alpha=0.5)
    - diversity_efficiency_ratio  # DER
    
  # Final evaluation only (at end of training)
  final:
    - all_evaluation_metrics
    - solution_histogram
    - objective_space_plot
    - trajectory_length_distribution
    - state_visitation_histogram

# ============================================================================
# ANALYSIS PLAN
# ============================================================================

analysis:
  
  statistical_tests:
    
    # Main effect: Does loss type affect diversity?
    - name: "base_loss_effect"
      test: "one_way_anova"
      groups: ["base_loss"]
      metric: "mode_coverage_entropy"
      posthoc: "tukey_hsd"
      alpha: 0.05
      
    # Effect size
    - name: "base_loss_effect_size"
      test: "cohens_d"
      comparisons: "all_pairs"
      metric: "mode_coverage_entropy"
      
    # Regularization effect
    - name: "regularization_effect"
      test: "one_way_anova"
      groups: ["regularization"]
      metric: "mode_coverage_entropy"
      
    # Quality-diversity tradeoff
    - name: "quality_diversity_tradeoff"
      test: "correlation"
      x_metric: "hypervolume"
      y_metric: "mode_coverage_entropy"
      method: "pearson"
  
  visualizations:
    
    - name: "loss_comparison_boxplot"
      type: "boxplot"
      x: "base_loss"
      y: "mode_coverage_entropy"
      hue: null
      save: "figures/loss_comparison.pdf"
      
    - name: "regularization_effect"
      type: "barplot"
      x: "regularization"
      y: "mode_coverage_entropy"
      save: "figures/regularization_effect.pdf"
      
    - name: "quality_diversity_scatter"
      type: "scatter"
      x: "hypervolume"
      y: "mode_coverage_entropy"
      hue: "base_loss"
      save: "figures/quality_diversity_tradeoff.pdf"
      
    - name: "diversity_evolution"
      type: "lineplot"
      x: "iteration"
      y: "mode_coverage_entropy"
      hue: "base_loss"
      save: "figures/diversity_evolution.pdf"
      
    - name: "loss_components_heatmap"
      type: "heatmap"
      pivot:
        index: "base_loss"
        columns: "regularization"
        values: "mode_coverage_entropy"
      save: "figures/loss_components_heatmap.pdf"

# ============================================================================
# EXPECTED RESULTS & HYPOTHESES
# ============================================================================

hypotheses:
  
  h1:
    statement: "SubTB(λ=0.9) produces higher diversity than TB, FM, or DB"
    prediction: "SubTB(0.9) MCE > TB MCE by at least 15%"
    rationale: "SubTB interpolates between local and global credit assignment"
    
  h2:
    statement: "Entropy regularization significantly increases diversity"
    prediction: "entropy(β=0.05) increases MCE by 20-30%"
    rationale: "Encourages policy exploration"
    
  h3:
    statement: "Too much regularization hurts quality"
    prediction: "entropy(β=0.5) reduces hypervolume by >10%"
    rationale: "Over-regularization reduces focus on high-reward regions"
    
  h4: # From Group 2
    statement: "SubTB(0.9) + entropy(β=0.05) is optimal combination"
    prediction: "Highest QDS score (balanced quality-diversity)"
    rationale: "SubTB provides good credit assignment, entropy adds exploration"
    
  h5:
    statement: "Loss function effects are independent of capacity/sampling"
    prediction: "Ranking of losses consistent across different model sizes"
    rationale: "Loss affects optimization dynamics universally"

# ============================================================================
# COMPUTATIONAL RESOURCES
# ============================================================================

resources:
  
  compute:
    platform: "slurm_cluster"  # or "local", "aws", "gcp"
    gpu_type: "nvidia_rtx3090"
    num_gpus_per_job: 1
    max_parallel_jobs: 10
    
  storage:
    base_dir: "./experiments/loss_ablation"
    structure:
      logs: "logs/"
      checkpoints: "checkpoints/"
      results: "results/"
      figures: "figures/"
      
  time_estimates:
    per_run: "24 minutes"  # HyperGrid is fast
    per_config: "2 hours"  # 5 seeds × 24 min
    total: "70 hours"  # 35 configs × 2 hours
    with_parallelization: "7 hours"  # With 10 parallel jobs

# ============================================================================
# EXECUTION PLAN
# ============================================================================

execution:
  
  priority:
    # Run in this order to inform later experiments
    - group: "base_loss_comparison"      # Day 1-2
    - group: "entropy_regularization"    # Day 3
    - group: "kl_regularization"         # Day 4
    - group: "subtb_entropy_sweep"       # Day 5
    - group: "loss_modifications"        # Day 6
    
  parallelization:
    strategy: "seed_parallel"  # Run 5 seeds in parallel
    max_parallel: 10
    
  checkpointing:
    save_every: 1000
    keep_last_n: 3
    
  logging:
    wandb:
      enabled: true
      project: "diversity-mogfn"
      entity: "your-team"
      tags: ["loss_ablation", "hypergrid", "week5"]
    
    local:
      log_level: "INFO"
      save_plots: true
      save_samples: true

# ============================================================================
# SUCCESS CRITERIA
# ============================================================================

success_criteria:
  
  minimum:
    - "All 35 configurations run successfully"
    - "At least 4/5 seeds complete per configuration"
    - "Statistical significance (p < 0.05) detected for at least 2 factors"
    
  good:
    - "Clear ranking of loss functions established"
    - "Optimal regularization strength identified"
    - "Quality-diversity tradeoff quantified"
    - "All metrics computed successfully"
    
  excellent:
    - "Novel insights discovered (e.g., unexpected interactions)"
    - "Best loss configuration improves diversity by >25% over baseline"
    - "Findings consistent with theoretical predictions"
    - "Results inform factorial experiments"

# ============================================================================
# NOTES & SPECIAL CONSIDERATIONS
# ============================================================================

notes:
  
  important:
    - "Run Group 1 (base_loss_comparison) FIRST - results inform later groups"
    - "Update 'best base loss' in Groups 2-5 based on Group 1 results"
    - "If TB and SubTB(0.9) are close, run both in later groups"
    - "Monitor training stability - some loss combinations may diverge"
    
  tips:
    - "Use early stopping if loss explodes (gradient_norm > 100)"
    - "Save samples at iterations [1000, 2000, 3000, 4000] for analysis"
    - "Generate GIFs showing diversity evolution over training"
    - "Track temperature of policy (entropy) over training"
    
  known_issues:
    - "DB can be unstable on hypergrid - increase gradient clipping if needed"
    - "Very high entropy regularization (β > 0.5) may prevent convergence"
    - "Reward shaping may need task-specific tuning"
    
  adaptive_strategy:
    - "If Group 1 shows TB >> SubTB, focus Groups 2-4 on TB only"
    - "If entropy clearly helps, expand Group 2 to test β ∈ [0.001, 1.0]"
    - "If no regularization helps, investigate why (mode collapse?)"

# ============================================================================
# OUTPUT DELIVERABLES
# ============================================================================

deliverables:
  
  week_5:
    - "results/loss_ablation_summary.csv"       # All metrics, all configs
    - "figures/loss_comparison.pdf"             # Main results figure
    - "figures/quality_diversity_tradeoff.pdf"  # Pareto frontier
    - "figures/regularization_effect.pdf"       # Beta sweep
    - "analysis/statistical_tests.txt"          # ANOVA, posthoc tests
    - "analysis/best_configuration.yaml"        # Winner for factorial study
    
  paper_draft:
    section: "5.2.3 Loss Function Effects"
    figures:
      - "Figure 5: Quality-diversity tradeoff for different losses"
    tables:
      - "Table X: Loss function comparison (all metrics)"
    key_findings:
      - "Best base loss identified"
      - "Optimal regularization strength"
      - "Effect sizes quantified"

# ============================================================================
# INTEGRATION WITH OTHER ABLATIONS
# ============================================================================

integration:
  
  from_previous_ablations:
    capacity_ablation:
      winner: "film_medium_64"
      use_in: "fixed_config.capacity"
      
    sampling_ablation:
      winner: "dirichlet_1.5"
      use_in: "fixed_config.dirichlet_alpha"
  
  to_factorial_experiments:
    export:
      - best_base_loss
      - best_regularization
      - best_modification
    use_in: "factorial_experiments/configs/loss_factor.yaml"
    
  expected_outcome:
    best_loss: "subtrajectory_balance_09"
    best_regularization: "entropy_005"
    best_modification: "standard"
    combined: "SubTB(0.9) + entropy(β=0.05)"