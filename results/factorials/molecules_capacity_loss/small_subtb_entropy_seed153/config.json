{
  "task": "ngrams",
  "vocab_size": 4,
  "seq_length": 8,
  "ngram_length": 2,
  "normalize_rewards": true,
  "temperature": 2.0,
  "sampling_strategy": "categorical",
  "preference_distribution": "dirichlet",
  "dirichlet_alpha": 1.5,
  "num_preferences_per_batch": 16,
  "max_iterations": 8000,
  "batch_size": 128,
  "optimizer": "adam",
  "learning_rate": 0.001,
  "gradient_clip": 10.0,
  "eval_every": 500,
  "eval_samples": 1000,
  "final_eval_samples": 10000,
  "num_seeds": 5,
  "base_seed": 42,
  "condition_name": "small_subtb_entropy",
  "hidden_dim": 64,
  "num_layers": 2,
  "conditioning": "concat",
  "activation": "relu",
  "num_parameters": "~1.3K",
  "factor_capacity": "small",
  "capacity_level": "small",
  "loss_function": "subtrajectory_balance",
  "loss_params": {
    "lambda_": 0.9,
    "log_reward_clip": 10.0
  },
  "regularization": "entropy",
  "regularization_params": {
    "beta": 0.01
  },
  "modifications": "none",
  "modifications_params": {},
  "factor_loss": "subtb_entropy",
  "loss_level": "subtb_entropy",
  "seed": 153,
  "exp_name": "small_subtb_entropy_seed153"
}