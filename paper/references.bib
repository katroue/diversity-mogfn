% Original GFLowNet paper
@inproceedings{zhang2022generative,
  title={Generative flow networks for discrete probabilistic modeling},
  author={Zhang, Dinghuai and Malkin, Nikolay and Liu, Zhen and Volokhova, Alexandra and Courville, Aaron and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={26412--26428},
  year={2022},
  organization={PMLR}
}

% Multi-Objective GFlowNets paper
@misc{jain2023multiobjectivegflownets,
      title={Multi-Objective GFlowNets}, 
      author={Moksh Jain and Sharath Chandra Raparthy and Alex Hernandez-Garcia and Jarrid Rector-Brooks and Yoshua Bengio and Santiago Miret and Emmanuel Bengio},
      year={2023},
      eprint={2210.12765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.12765}, 
}

@misc{zhu2023sampleefficientmultiobjectivemolecularoptimization,
      title={Sample-efficient Multi-objective Molecular Optimization with GFlowNets}, 
      author={Yiheng Zhu and Jialu Wu and Chaowen Hu and Jiahuan Yan and Chang-Yu Hsieh and Tingjun Hou and Jian Wu},
      year={2023},
      eprint={2302.04040},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.04040}, 
}

@misc{bengio2023gflownetfoundations,
      title={GFlowNet Foundations}, 
      author={Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
      year={2023},
      eprint={2111.09266},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09266}, 
}

@misc{malkin2023trajectorybalanceimprovedcredit,
      title={Trajectory balance: Improved credit assignment in GFlowNets}, 
      author={Nikolay Malkin and Moksh Jain and Emmanuel Bengio and Chen Sun and Yoshua Bengio},
      year={2023},
      eprint={2201.13259},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.13259}, 
}

@misc{hu2024squarederrorexploringloss,
      title={Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks}, 
      author={Rui Hu and Yifan Zhang and Zhuoran Li and Longbo Huang},
      year={2024},
      eprint={2410.02596},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02596}, 
}

@misc{chen2025gradientbasedmultiobjectivedeeplearning,
      title={Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond}, 
      author={Weiyu Chen and Baijiong Lin and Xiaoyuan Zhang and Xi Lin and Han Zhao and Qingfu Zhang and James T. Kwok},
      year={2025},
      eprint={2501.10945},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.10945}, 
}

@misc{lin2024smoothtchebycheffscalarizationmultiobjective,
      title={Smooth Tchebycheff Scalarization for Multi-Objective Optimization}, 
      author={Xi Lin and Xiaoyuan Zhang and Zhiyuan Yang and Fei Liu and Zhenkun Wang and Qingfu Zhang},
      year={2024},
      eprint={2402.19078},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19078}, 
}

@misc{ye2024datadrivenpreferencesamplingpareto,
      title={Data-Driven Preference Sampling for Pareto Front Learning}, 
      author={Rongguang Ye and Lei Chen and Weiduo Liao and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2404.08397},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.08397}, 
}

@misc{dimitriadis2025paretolowrankadaptersefficient,
      title={Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences}, 
      author={Nikolaos Dimitriadis and Pascal Frossard and Francois Fleuret},
      year={2025},
      eprint={2407.08056},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08056}, 
}

@misc{ye2024paretoshapeagnosticparetoset,
      title={Pareto Front Shape-Agnostic Pareto Set Learning in Multi-Objective Optimization}, 
      author={Rongguang Ye and Longcan Chen and Wei-Bin Kou and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2408.05778},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05778}, 
}

@article{Guerreiro_2021,
   title={The Hypervolume Indicator: Computational Problems and Algorithms},
   volume={54},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3453474},
   DOI={10.1145/3453474},
   number={6},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Guerreiro, Andreia P. and Fonseca, Carlos M. and Paquete, Luís},
   year={2021},
   month=jul, pages={1–42} }

@misc{schäpermeier2025r2v2paretocompliantr2,
      title={R2 v2: The Pareto-compliant R2 Indicator for Better Benchmarking in Bi-objective Optimization}, 
      author={Lennart Schäpermeier and Pascal Kerschke},
      year={2025},
      eprint={2407.01504},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2407.01504}, 
}

@InProceedings{pmlr-v202-madan23a,
  title = 	 {Learning {GF}low{N}ets From Partial Episodes For Improved Convergence And Stability},
  author =       {Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Nica, Andrei Cristian and Bosc, Tom and Bengio, Yoshua and Malkin, Nikolay},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {23467--23483},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/madan23a/madan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/madan23a.html},
  abstract = 	 {Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\lambda$) algorithm in reinforcement learning, we introduce <em>subtrajectory balance</em> or SubTB($\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.}
}


% Key GFlowNet Papers
@inproceedings{bengio2021flow,
  title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author={Bengio, Emmanuel and others},
  booktitle={NeurIPS},
  year={2021}
}

% Quality-Diversity
@article{pugh2016quality,
  title={Quality Diversity: A New Frontier for Evolutionary Computation},
  author={Pugh, Justin K and others},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016}
}

@ARTICLE{7959075,
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Quality and Diversity Optimization: A Unifying Modular Framework}, 
  year={2018},
  volume={22},
  number={2},
  pages={245-259},
  keywords={Optimization;Legged locomotion;Sociology;Statistics;Algorithm design and analysis;Evolutionary computation;Behavioral diversity;collection of solutions;novelty search;optimization methods;quality-diversity (QD)},
  doi={10.1109/TEVC.2017.2704781}}


% Information Theory
@book{cover2006elements,
  title={Elements of Information Theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2006},
  publisher={Wiley}
}

% Levenstein Distance
@article{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  journal={Soviet Physics Doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966}
}

% Gini Coefficient
@article{gini1912variability,
  title={Variability and Mutability},
  author={Gini, Corrado},
  journal={Memorie di metodologica statistica},
  year={1912}
}

% Traditional MOO Metrics
@article{zitzler1999multiobjective,
  title={Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  author={Zitzler, Eckart and Thiele, Lothar},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={3},
  number={4},
  pages={257--271},
  year={1999}
}

% Hypervolume metric
@article{10.1145/3453474,
author = {Guerreiro, Andreia P. and Fonseca, Carlos M. and Paquete, Lu\'{\i}s},
title = {The Hypervolume Indicator: Computational Problems and Algorithms},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453474},
doi = {10.1145/3453474},
abstract = {The hypervolume indicator is one of the most used set-quality indicators for the assessment of stochastic multiobjective optimizers, as well as for selection in evolutionary multiobjective optimization algorithms. Its theoretical properties justify its wide acceptance, particularly the strict monotonicity with respect to set dominance, which is still unique of hypervolume-based indicators. This article discusses the computation of hypervolume-related problems, highlighting the relations between them, providing an overview of the paradigms and techniques used, a description of the main algorithms for each problem, and a rundown of the fastest algorithms regarding asymptotic complexity and runtime. By providing a complete overview of the computational problems associated to the hypervolume indicator, this article serves as the starting point for the development of new algorithms and supports users in the identification of the most appropriate implementations available for each problem.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {119},
numpages = {42},
keywords = {multiobjective optimization, hypervolume subset selection problem, hypervolume contributions, Hypervolume indicator}
}

% FiLM Conditioning
@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

% Temperature (Sampling strategy)
@article{zhang2023robust,
  title={Robust scheduling with gflownets},
  author={Zhang, David W and Rainone, Corrado and Peschl, Markus and Bondesan, Roberto},
  journal={arXiv preprint arXiv:2302.05446},
  year={2023}
}

@article{kim2023learning,
  title={Learning to scale logits for temperature-conditional gflownets},
  author={Kim, Minsu and Ko, Joohwan and Yun, Taeyoung and Zhang, Dinghuai and Pan, Ling and Kim, Woochang and Park, Jinkyoo and Bengio, Emmanuel and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2310.02823},
  year={2023}
}

% Sampling Strategy : Greedy (Deterministic)
@article{doi:10.1177/0278364917714338,
author = {Lucas Janson and Brian Ichter and Marco Pavone},
title ={Deterministic sampling-based motion planning: Optimality, complexity, and performance},
journal = {The International Journal of Robotics Research},
volume = {37},
number = {1},
pages = {46-61},
year = {2018},
doi = {10.1177/0278364917714338},
URL = {https://doi.org/10.1177/0278364917714338},
eprint = {https://doi.org/10.1177/0278364917714338}
}

% Sampling Strategy : Categorical
@inproceedings{bengio2021flow,
  title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author={Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

% Sampling stategy : Top-K and Top-P (Neucleus)
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

% Factorial Experiments
@book{box1978statistics,
  title={Statistics for experimenters},
  author={Box, George EP and Hunter, William H and Hunter, Stuart and others},
  volume={664},
  year={1978},
  publisher={John Wiley and sons New York}
}

@book{montgomery2017design,
  title={Design and analysis of experiments},
  author={Montgomery, Douglas C},
  year={2017},
  publisher={John wiley \& sons}
}

@article{bouthillier2021accounting,
  title={Accounting for variance in machine learning benchmarks},
  author={Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={747--769},
  year={2021}
}

% N-Grams task
@InProceedings{pmlr-v162-stanton22a,
  title = 	 {Accelerating {B}ayesian Optimization for Biological Sequence Design with Denoising Autoencoders},
  author =       {Stanton, Samuel and Maddox, Wesley and Gruver, Nate and Maffettone, Phillip and Delaney, Emily and Greenside, Peyton and Wilson, Andrew Gordon},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {20459--20478},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/stanton22a/stanton22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/stanton22a.html},
  abstract = 	 {Bayesian optimization (BayesOpt) is a gold standard for query-efficient continuous optimization. However, its adoption for drug design has been hindered by the discrete, high-dimensional nature of the decision variables. We develop a new approach (LaMBO) which jointly trains a denoising autoencoder with a discriminative multi-task Gaussian process head, allowing gradient-based optimization of multi-objective acquisition functions in the latent space of the autoencoder. These acquisition functions allow LaMBO to balance the explore-exploit tradeoff over multiple design rounds, and to balance objective tradeoffs by optimizing sequences at many different points on the Pareto frontier. We evaluate LaMBO on two small-molecule design tasks, and introduce new tasks optimizing in silico and in vitro properties of large-molecule fluorescent proteins. In our experiments LaMBO outperforms genetic optimizers and does not require a large pretraining corpus, demonstrating that BayesOpt is practical and effective for biological sequence design.}
}

% Fragment-based
@article{kumar2012fragment,
  title={Fragment based drug design: from experimental to computational approaches},
  author={Kumar, Ashutosh and Voet, Arnout and Zhang, Kam YJ},
  journal={Current medicinal chemistry},
  volume={19},
  number={30},
  pages={5128--5147},
  year={2012},
  publisher={Bentham Science Publishers direct}
}

% Adam optimization
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% NSGA-II MOO algorithm
@article{deb2002fast,
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author={Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal={IEEE transactions on evolutionary computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002},
  publisher={Ieee}
}

% NSGA-III
@article{deb2013evolutionary,
  title={An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: solving problems with box constraints},
  author={Deb, Kalyanmoy and Jain, Himanshu},
  journal={IEEE transactions on evolutionary computation},
  volume={18},
  number={4},
  pages={577--601},
  year={2013},
  publisher={IEEE}
}

% MOEA/D MOO algorithm
@article{zhang2007moea,
  title={MOEA/D: A multiobjective evolutionary algorithm based on decomposition},
  author={Zhang, Qingfu and Li, Hui},
  journal={IEEE Transactions on evolutionary computation},
  volume={11},
  number={6},
  pages={712--731},
  year={2007},
  publisher={IEEE}
}

% Hypernetwork GFlowNet (HN-GFN)
@article{zhu2023sample,
  title={Sample-efficient multi-objective molecular optimization with gflownets},
  author={Zhu, Yiheng and Wu, Jialu and Hu, Chaowen and Yan, Jiahuan and Hou, Tingjun and Wu, Jian and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={79667--79684},
  year={2023}
}

% Crystal GFlowNet
@article{ai4science2023crystal,
  title={Crystal-gfn: sampling crystals with desirable properties and constraints},
  author={AI4Science, Mila and Hernandez-Garcia, Alex and Duval, Alexandre and Volokhova, Alexandra and Bengio, Yoshua and Sharma, Divya and Carrier, Pierre Luc and Benabed, Yasmine and Koziarski, Micha{\l} and Schmidt, Victor},
  journal={arXiv preprint arXiv:2310.04925},
  year={2023}
}

@article{hernandez2023multi,
  title={Multi-fidelity active learning with gflownets},
  author={Hernandez-Garcia, Alex and Saxena, Nikita and Jain, Moksh and Liu, Cheng-Hao and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2306.11715},
  year={2023}
}

@article{jain2023gflownets,
  title={Gflownets for ai-driven scientific discovery},
  author={Jain, Moksh and Deleu, Tristan and Hartford, Jason and Liu, Cheng-Hao and Hernandez-Garcia, Alex and Bengio, Yoshua},
  journal={Digital Discovery},
  volume={2},
  number={3},
  pages={557--577},
  year={2023},
  publisher={Royal Society of Chemistry}
}

% Training stability and convergence
@inproceedings{madan2023learning,
  title={Learning GFlowNets from partial episodes for improved convergence and stability},
  author={Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Maksymets, Andrei and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={23467--23483},
  year={2023}
}

% Exploration challenges
@inproceedings{pan2023better,
  title={Better training of GFlowNets with local credit and incomplete trajectories},
  author={Pan, Ling and Malkin, Nikolay and Zhang, Dinghuai and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={26878--26890},
  year={2023}
}

% Mode collapse and diversity challenges
@inproceedings{zhang2023gflownet,
  title={Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets},
  author={Zhang, Dinghuai and Dai, Hanjun and Malkin, Nikolay and Courville, Aaron and Bengio, Yoshua and Jain, Moksh},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% QED Objective (Molecules)
@article{bickerton2012quantifying,
  title={Quantifying the chemical beauty of drugs},
  author={Bickerton, G Richard and Paolini, Gaia V and Besnard, J{\'e}r{\'e}my and Muresan, Sorel and Hopkins, Andrew L},
  journal={Nature chemistry},
  volume={4},
  number={2},
  pages={90--98},
  year={2012},
  publisher={Nature Publishing Group UK London}
}

% Synthetic accessibility objective (molecules)
@article{ertl2009estimation,
  title={Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions},
  author={Ertl, Peter and Schuffenhauer, Ansgar},
  journal={Journal of cheminformatics},
  volume={1},
  number={1},
  pages={8},
  year={2009},
  publisher={Springer}
}

% logP objective (molecules)
@article{wildman1999prediction,
  title={Prediction of physicochemical parameters by atomic contributions},
  author={Wildman, Scott A and Crippen, Gordon M},
  journal={Journal of chemical information and computer sciences},
  volume={39},
  number={5},
  pages={868--873},
  year={1999},
  publisher={ACS Publications}
}

% Diversity metric in Biological design using gflownets
@inproceedings{jain2022biological,
  title={Biological sequence design with gflownets},
  author={Jain, Moksh and Bengio, Emmanuel and Hernandez-Garcia, Alex and Rector-Brooks, Jarrid and Dossou, Bonaventure FP and Ekbote, Chanakya Ajit and Fu, Jie and Zhang, Tianyu and Kilgour, Michael and Zhang, Dinghuai and others},
  booktitle={International Conference on Machine Learning},
  pages={9786--9801},
  year={2022},
  organization={PMLR}
}

% Spacing metric
@mastersthesis{schott1995fault,
  title={Fault Tolerant Design Using Single and Multicriteria Genetic Algorithm Optimization},
  author={Schott, John R.},
  year={1995},
  school={Massachusetts Institute of Technology}
}