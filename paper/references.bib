% Multi-Objective GFlowNets paper
@misc{jain2023multiobjectivegflownets,
      title={Multi-Objective GFlowNets}, 
      author={Moksh Jain and Sharath Chandra Raparthy and Alex Hernandez-Garcia and Jarrid Rector-Brooks and Yoshua Bengio and Santiago Miret and Emmanuel Bengio},
      year={2023},
      eprint={2210.12765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.12765}, 
}

@misc{zhu2023sampleefficientmultiobjectivemolecularoptimization,
      title={Sample-efficient Multi-objective Molecular Optimization with GFlowNets}, 
      author={Yiheng Zhu and Jialu Wu and Chaowen Hu and Jiahuan Yan and Chang-Yu Hsieh and Tingjun Hou and Jian Wu},
      year={2023},
      eprint={2302.04040},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.04040}, 
}

@misc{bengio2023gflownetfoundations,
      title={GFlowNet Foundations}, 
      author={Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
      year={2023},
      eprint={2111.09266},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.09266}, 
}

@misc{malkin2023trajectorybalanceimprovedcredit,
      title={Trajectory balance: Improved credit assignment in GFlowNets}, 
      author={Nikolay Malkin and Moksh Jain and Emmanuel Bengio and Chen Sun and Yoshua Bengio},
      year={2023},
      eprint={2201.13259},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.13259}, 
}

@misc{hu2024squarederrorexploringloss,
      title={Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks}, 
      author={Rui Hu and Yifan Zhang and Zhuoran Li and Longbo Huang},
      year={2024},
      eprint={2410.02596},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.02596}, 
}

@misc{chen2025gradientbasedmultiobjectivedeeplearning,
      title={Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond}, 
      author={Weiyu Chen and Baijiong Lin and Xiaoyuan Zhang and Xi Lin and Han Zhao and Qingfu Zhang and James T. Kwok},
      year={2025},
      eprint={2501.10945},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.10945}, 
}

@misc{lin2024smoothtchebycheffscalarizationmultiobjective,
      title={Smooth Tchebycheff Scalarization for Multi-Objective Optimization}, 
      author={Xi Lin and Xiaoyuan Zhang and Zhiyuan Yang and Fei Liu and Zhenkun Wang and Qingfu Zhang},
      year={2024},
      eprint={2402.19078},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19078}, 
}

@misc{ye2024datadrivenpreferencesamplingpareto,
      title={Data-Driven Preference Sampling for Pareto Front Learning}, 
      author={Rongguang Ye and Lei Chen and Weiduo Liao and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2404.08397},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.08397}, 
}

@misc{dimitriadis2025paretolowrankadaptersefficient,
      title={Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences}, 
      author={Nikolaos Dimitriadis and Pascal Frossard and Francois Fleuret},
      year={2025},
      eprint={2407.08056},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08056}, 
}

@misc{ye2024paretoshapeagnosticparetoset,
      title={Pareto Front Shape-Agnostic Pareto Set Learning in Multi-Objective Optimization}, 
      author={Rongguang Ye and Longcan Chen and Wei-Bin Kou and Jinyuan Zhang and Hisao Ishibuchi},
      year={2024},
      eprint={2408.05778},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05778}, 
}

@article{Guerreiro_2021,
   title={The Hypervolume Indicator: Computational Problems and Algorithms},
   volume={54},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3453474},
   DOI={10.1145/3453474},
   number={6},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Guerreiro, Andreia P. and Fonseca, Carlos M. and Paquete, Luís},
   year={2021},
   month=jul, pages={1–42} }

@misc{schäpermeier2025r2v2paretocompliantr2,
      title={R2 v2: The Pareto-compliant R2 Indicator for Better Benchmarking in Bi-objective Optimization}, 
      author={Lennart Schäpermeier and Pascal Kerschke},
      year={2025},
      eprint={2407.01504},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2407.01504}, 
}

@InProceedings{pmlr-v202-madan23a,
  title = 	 {Learning {GF}low{N}ets From Partial Episodes For Improved Convergence And Stability},
  author =       {Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Nica, Andrei Cristian and Bosc, Tom and Bengio, Yoshua and Malkin, Nikolay},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {23467--23483},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/madan23a/madan23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/madan23a.html},
  abstract = 	 {Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\lambda$) algorithm in reinforcement learning, we introduce <em>subtrajectory balance</em> or SubTB($\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.}
}


% Key GFlowNet Papers
@inproceedings{bengio2021flow,
  title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author={Bengio, Emmanuel and others},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{jain2023multi,
  title={Multi-Objective {GFlowNets}},
  author={Jain, Moksh and others},
  booktitle={ICML},
  year={2023}
}

% Quality-Diversity
@article{pugh2016quality,
  title={Quality Diversity: A New Frontier for Evolutionary Computation},
  author={Pugh, Justin K and others},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016}
}

@ARTICLE{7959075,
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Quality and Diversity Optimization: A Unifying Modular Framework}, 
  year={2018},
  volume={22},
  number={2},
  pages={245-259},
  keywords={Optimization;Legged locomotion;Sociology;Statistics;Algorithm design and analysis;Evolutionary computation;Behavioral diversity;collection of solutions;novelty search;optimization methods;quality-diversity (QD)},
  doi={10.1109/TEVC.2017.2704781}}


% Information Theory
@book{cover2006elements,
  title={Elements of Information Theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2006},
  publisher={Wiley}
}

% Edit Distance
@article{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  journal={Soviet Physics Doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966}
}

% Gini Coefficient
@article{gini1912variability,
  title={Variability and Mutability},
  author={Gini, Corrado},
  journal={Memorie di metodologica statistica},
  year={1912}
}

% Traditional MOO Metrics
@article{zitzler1999multiobjective,
  title={Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach},
  author={Zitzler, Eckart and Thiele, Lothar},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={3},
  number={4},
  pages={257--271},
  year={1999}
}

@article{deb2002fast,
  title={A fast and elitist multiobjective genetic algorithm: {NSGA-II}},
  author={Deb, Kalyanmoy and others},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002}
}

% Hypervolume metric
@article{10.1145/3453474,
author = {Guerreiro, Andreia P. and Fonseca, Carlos M. and Paquete, Lu\'{\i}s},
title = {The Hypervolume Indicator: Computational Problems and Algorithms},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453474},
doi = {10.1145/3453474},
abstract = {The hypervolume indicator is one of the most used set-quality indicators for the assessment of stochastic multiobjective optimizers, as well as for selection in evolutionary multiobjective optimization algorithms. Its theoretical properties justify its wide acceptance, particularly the strict monotonicity with respect to set dominance, which is still unique of hypervolume-based indicators. This article discusses the computation of hypervolume-related problems, highlighting the relations between them, providing an overview of the paradigms and techniques used, a description of the main algorithms for each problem, and a rundown of the fastest algorithms regarding asymptotic complexity and runtime. By providing a complete overview of the computational problems associated to the hypervolume indicator, this article serves as the starting point for the development of new algorithms and supports users in the identification of the most appropriate implementations available for each problem.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {119},
numpages = {42},
keywords = {multiobjective optimization, hypervolume subset selection problem, hypervolume contributions, Hypervolume indicator}
}

% FiLM Conditioning
@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

% Temperature (Sampling strategy)
@article{zhang2023robust,
  title={Robust scheduling with gflownets},
  author={Zhang, David W and Rainone, Corrado and Peschl, Markus and Bondesan, Roberto},
  journal={arXiv preprint arXiv:2302.05446},
  year={2023}
}

@article{kim2023learning,
  title={Learning to scale logits for temperature-conditional gflownets},
  author={Kim, Minsu and Ko, Joohwan and Yun, Taeyoung and Zhang, Dinghuai and Pan, Ling and Kim, Woochang and Park, Jinkyoo and Bengio, Emmanuel and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2310.02823},
  year={2023}
}

% Sampling Strategy : Greedy (Deterministic)
@article{doi:10.1177/0278364917714338,
author = {Lucas Janson and Brian Ichter and Marco Pavone},
title ={Deterministic sampling-based motion planning: Optimality, complexity, and performance},
journal = {The International Journal of Robotics Research},
volume = {37},
number = {1},
pages = {46-61},
year = {2018},
doi = {10.1177/0278364917714338},
URL = {https://doi.org/10.1177/0278364917714338},
eprint = {https://doi.org/10.1177/0278364917714338}
}

% Sampling Strategy : Categorical
@inproceedings{bengio2021flow,
  title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
  author={Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

% Sampling stategy : Top-K and Top-P (Neucleus)
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}